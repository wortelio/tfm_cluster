{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "225b20d5-15c5-4de6-b91b-228405789ed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import datetime\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "from collections import Counter # for mAP\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "# Not needed\n",
    "# from torchvision.transforms import ToTensor \n",
    "# from torchvision.transforms.v2 import (Compose, ToImage, ToDtype, \n",
    "#                                        Normalize, RandomPhotometricDistort)\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch.nn as nn \n",
    "from torch.nn.utils import parameters_to_vector\n",
    "import torch.optim as optim\n",
    "\n",
    "from torchvision.ops import complete_box_iou_loss, distance_box_iou_loss\n",
    "\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cbcd1701-efe7-42a5-a76e-2f511ea049c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a744c25c-0962-4cc8-986f-2ca69aca323e",
   "metadata": {},
   "source": [
    "# Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4072248-8bb2-4cb7-a71b-7b4586d8b188",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_path = 'bed_pytorch_results/box_loss_ciou_nan/'\n",
    "\n",
    "logger = logging.getLogger(\"GonLogger\")\n",
    "logger.propagate = False\n",
    "logger.setLevel(logging.INFO)\n",
    "file_handler = logging.FileHandler(log_path + 'logfile.log')\n",
    "formatter = logging.Formatter('%(message)s')\n",
    "file_handler.setFormatter(formatter)\n",
    "\n",
    "# add file handler to logger\n",
    "logger.addHandler(file_handler)\n",
    "\n",
    "logger.info('OPTIM BED after reducing model and dropout, adding Loss Regularization and Weight Decay and BOX CIOU Loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c28235c-9cc5-4676-b2b1-c31b3c214e83",
   "metadata": {},
   "source": [
    "# Folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af468cf3-f6ef-4eff-9713-3034a68830c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dir: ['labels - copia.cache', 'labels.cache', 'labels', 'images']\n",
      "val dir: ['.ipynb_checkpoints', 'labels', 'images']\n"
     ]
    }
   ],
   "source": [
    "ds_dir = 'ds2fire/dfire_yolo/'\n",
    "\n",
    "train_dir = ds_dir + 'train/'\n",
    "train_imgs = train_dir + 'images/'\n",
    "train_labels = train_dir + 'labels/'\n",
    "\n",
    "#val_dir = ds_dir + 'test/'\n",
    "\n",
    "val_dir = ds_dir + 'test/'\n",
    "val_imgs = val_dir + 'images/'\n",
    "val_labels = val_dir + 'labels/'\n",
    "\n",
    "print(f'Train dir: {os.listdir(train_dir)}')\n",
    "print(f'val dir: {os.listdir(val_dir)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676dbf6f-f393-4b79-8d23-bda75a58caea",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f92b6578-e873-4f1b-a47e-b0788f9c612a",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASSES = [\"smoke\", \"fire\"]\n",
    "#CLASSES = [\"cat\", \"dog\"]\n",
    "\n",
    "#IMG_DIM = {'W':88, 'H':88} # (W, H)\n",
    "#IMG_DIM = {'W':160, 'H':120} # (W, H)\n",
    "#IMG_DIM = {'W':448, 'H':448} # (W, H)\n",
    "IMG_DIM = {'W':224, 'H':224} # (W, H)\n",
    "\n",
    "SX = 7\n",
    "SY = 7\n",
    "B = 2 # Number of bounding boxes to predict.\n",
    "C = len(CLASSES) # Number of classes in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8704ff2a-2926-4fc7-b2fc-ac3a3fcff843",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b30097d-3161-4cc1-81fa-ebdf9f0c4e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def yolo2pixel(bbox):\n",
    "    '''\n",
    "    Transforms yolo coordinates of the box to pixel coordinates. \n",
    "    \n",
    "    Arguments:\n",
    "        - bbox: yolo coordinates [xc, yc, width, height]\n",
    "    \n",
    "    Returns: \n",
    "        - pixel coordinates [xmin, xmax, ymin, ymax]\n",
    "    '''\n",
    "    xc = bbox[0]\n",
    "    yc = bbox[1]\n",
    "    width = bbox[2]\n",
    "    height = bbox[3]\n",
    "      \n",
    "    xmin = xc - (width/2)          \n",
    "    xmax = xc + (width/2)         \n",
    "    ymin = yc - (height/2)            \n",
    "    ymax = yc + (height/2)\n",
    "        \n",
    "    nbox = [xmin, ymin, xmax, ymax]\n",
    "    \n",
    "    return nbox\n",
    "\n",
    "\n",
    "def iou_tensor(\n",
    "    boxes_preds, boxes_labels, \n",
    "    box_format=\"midpoint\",\n",
    "    epsilon=1e-6\n",
    "):\n",
    "    \"\"\"\n",
    "    Calculates intersection over union for bounding boxes.\n",
    "    \n",
    "    :param boxes_preds (tensor): Bounding box predictions of shape (BATCH_SIZE, 4)\n",
    "    :param boxes_labels (tensor): Ground truth bounding box of shape (BATCH_SIZE, 4)\n",
    "    :param box_format (str): midpoint/corners, if boxes (x,y,w,h) format or (x1,y1,x2,y2) format\n",
    "    :param epsilon: Small value to prevent division by zero.\n",
    "    Returns:\n",
    "        tensor: Intersection over union for all examples\n",
    "    \"\"\"\n",
    "\n",
    "    if box_format == 'midpoint':\n",
    "        box1_x1 = boxes_preds[..., 0:1] - boxes_preds[..., 2:3] / 2\n",
    "        box1_y1 = boxes_preds[..., 1:2] - boxes_preds[..., 3:4] / 2\n",
    "        box1_x2 = boxes_preds[..., 0:1] + boxes_preds[..., 2:3] / 2\n",
    "        box1_y2 = boxes_preds[..., 1:2] + boxes_preds[..., 3:4] / 2\n",
    "        box2_x1 = boxes_labels[..., 0:1] - boxes_labels[..., 2:3] / 2\n",
    "        box2_y1 = boxes_labels[..., 1:2] - boxes_labels[..., 3:4] / 2\n",
    "        box2_x2 = boxes_labels[..., 0:1] + boxes_labels[..., 2:3] / 2\n",
    "        box2_y2 = boxes_labels[..., 1:2] + boxes_labels[..., 3:4] / 2\n",
    "\n",
    "    if box_format == 'corners':\n",
    "        box1_x1 = boxes_preds[..., 0:1]\n",
    "        box1_y1 = boxes_preds[..., 1:2]\n",
    "        box1_x2 = boxes_preds[..., 2:3]\n",
    "        box1_y2 = boxes_preds[..., 3:4] \n",
    "        box2_x1 = boxes_labels[..., 0:1]\n",
    "        box2_y1 = boxes_labels[..., 1:2]\n",
    "        box2_x2 = boxes_labels[..., 2:3]\n",
    "        box2_y2 = boxes_labels[..., 3:4]\n",
    "\n",
    "    x1 = torch.max(box1_x1, box2_x1)\n",
    "    y1 = torch.max(box1_y1, box2_y1)\n",
    "    x2 = torch.min(box1_x2, box2_x2)\n",
    "    y2 = torch.min(box1_y2, box2_y2)\n",
    "\n",
    "    intersection = (x2 - x1).clamp(0) * (y2 - y1).clamp(0)\n",
    "\n",
    "    box1_area = abs((box1_x2 - box1_x1) * (box1_y2 - box1_y1))\n",
    "    box2_area = abs((box2_x2 - box2_x1) * (box2_y2 - box2_y1))\n",
    "\n",
    "    union = (box1_area + box2_area - intersection + epsilon)\n",
    "\n",
    "    iou = intersection / union\n",
    "    #print(f'IOU is numpy: {iou.numpy()}')\n",
    "\n",
    "    return iou\n",
    "\n",
    "def nms_yv1(bboxes, iou_threshold, threshold, box_format=\"corners\"):\n",
    "    \"\"\"\n",
    "    Does Non Max Suppression given bboxes\n",
    "\n",
    "    Parameters:\n",
    "        bboxes (list): list of lists containing all bboxes with each bboxes\n",
    "        specified as [x1, y1, x2, y2, confidence, class_id] MY FORMAT VERSION       \n",
    "        iou_threshold (float): threshold where predicted bboxes is correct\n",
    "        threshold (float): threshold to remove predicted bboxes (independent of IoU) \n",
    "        box_format (str): \"midpoint\" or \"corners\" used to specify bboxes\n",
    "\n",
    "    Returns:\n",
    "        list: bboxes after performing NMS given a specific IoU threshold\n",
    "    \"\"\"\n",
    "\n",
    "    assert type(bboxes) == list\n",
    "\n",
    "    bboxes = [box for box in bboxes if box[4] > threshold]\n",
    "    bboxes = sorted(bboxes, key=lambda x: x[4], reverse=True)\n",
    "    bboxes_after_nms = []\n",
    "\n",
    "    while bboxes:\n",
    "        chosen_box = bboxes.pop(0)\n",
    "\n",
    "        bboxes = [\n",
    "            box\n",
    "            for box in bboxes\n",
    "            if box[5] != chosen_box[5]\n",
    "            or iou_tensor(\n",
    "                torch.tensor(chosen_box[:4]),\n",
    "                torch.tensor(box[:4]),\n",
    "                box_format=box_format,\n",
    "            )\n",
    "            < iou_threshold\n",
    "        ]\n",
    "\n",
    "        bboxes_after_nms.append(chosen_box)\n",
    "\n",
    "    return bboxes_after_nms\n",
    "\n",
    "def nms_yv1_getBBoxes(bboxes, iou_threshold, threshold, box_format=\"corners\"):\n",
    "    \"\"\"\n",
    "    Does Non Max Suppression given bboxes\n",
    "\n",
    "    Parameters:\n",
    "        bboxes (list): list of lists containing all bboxes with each bboxes\n",
    "        specified as [class_id, score, xc, yc, w, h] Output of outcell_2_outboxes       \n",
    "        iou_threshold (float): threshold where predicted bboxes is correct\n",
    "        threshold (float): threshold to remove predicted bboxes (independent of IoU) \n",
    "        box_format (str): \"midpoint\" or \"corners\" used to specify bboxes\n",
    "\n",
    "    Returns:\n",
    "        list: bboxes after performing NMS given a specific IoU threshold\n",
    "    \"\"\"\n",
    "\n",
    "    assert type(bboxes) == list\n",
    "\n",
    "    bboxes = [box for box in bboxes if box[1] > threshold]\n",
    "    bboxes = sorted(bboxes, key=lambda x: x[1], reverse=True)\n",
    "    bboxes_after_nms = []\n",
    "\n",
    "    while bboxes:\n",
    "        chosen_box = bboxes.pop(0)\n",
    "\n",
    "        bboxes = [\n",
    "            box\n",
    "            for box in bboxes\n",
    "            if box[0] != chosen_box[0]\n",
    "            or iou_tensor(\n",
    "                torch.tensor(chosen_box[2:6]),\n",
    "                torch.tensor(box[2:6]),\n",
    "                box_format=box_format,\n",
    "            )\n",
    "            < iou_threshold\n",
    "        ]\n",
    "\n",
    "        bboxes_after_nms.append(chosen_box)\n",
    "\n",
    "    return bboxes_after_nms\n",
    "\n",
    "\n",
    "\n",
    "def mAP(\n",
    "    pred_boxes, true_boxes, iou_threshold=0.5, box_format=\"midpoint\", num_classes=2\n",
    "):\n",
    "    \"\"\"\n",
    "    Calculates mean average precision \n",
    "\n",
    "    Parameters:\n",
    "        pred_boxes (list): list of lists containing all bboxes with each bboxes\n",
    "        specified as [train_idx, class_prediction, prob_score, x1, y1, x2, y2]\n",
    "        true_boxes (list): Similar as pred_boxes except all the correct ones \n",
    "        iou_threshold (float): threshold where predicted bboxes is correct\n",
    "        box_format (str): \"midpoint\" or \"corners\" used to specify bboxes\n",
    "        num_classes (int): number of classes\n",
    "\n",
    "    Returns:\n",
    "        float: mAP value across all classes given a specific IoU threshold \n",
    "    \"\"\"\n",
    "\n",
    "    # list storing all AP for respective classes\n",
    "    average_precisions = []\n",
    "    avg_prec = {}\n",
    "\n",
    "    # Precision and Recall for each class\n",
    "    cls_prec = {}\n",
    "    cls_rec = {}\n",
    "\n",
    "    # used for numerical stability later on\n",
    "    epsilon = 1e-6\n",
    "\n",
    "    #for c in range(num_classes):\n",
    "    for c in tqdm(range(num_classes), desc =\"mAP:@.5\"):\n",
    "        detections = []\n",
    "        ground_truths = []\n",
    "\n",
    "        # Go through all predictions and targets,\n",
    "        # and only add the ones that belong to the\n",
    "        # current class c\n",
    "        for detection in pred_boxes:\n",
    "            if detection[1] == c:\n",
    "                detections.append(detection)\n",
    "        \n",
    "        #print(f'Detections of class {c}: {detections}')\n",
    "        \n",
    "        for true_box in true_boxes:\n",
    "            if true_box[1] == c:\n",
    "                ground_truths.append(true_box)\n",
    "\n",
    "        # find the amount of bboxes for each training example\n",
    "        # Counter here finds how many ground truth bboxes we get\n",
    "        # for each training example, so let's say img 0 has 3,\n",
    "        # img 1 has 5 then we will obtain a dictionary with:\n",
    "        # amount_bboxes = {0:3, 1:5}\n",
    "        amount_bboxes = Counter([gt[0] for gt in ground_truths])\n",
    "        \n",
    "        #print(f'Amount bboxes of class {c}: {amount_bboxes}')\n",
    "\n",
    "        # We then go through each key, val in this dictionary\n",
    "        # and convert to the following (w.r.t same example):\n",
    "        # ammount_bboxes = {0:torch.tensor[0,0,0], 1:torch.tensor[0,0,0,0,0]}\n",
    "        for key, val in amount_bboxes.items():\n",
    "            amount_bboxes[key] = torch.zeros(val)\n",
    "\n",
    "        #print(f'Amount bboxes of class {c} converted: {amount_bboxes}')\n",
    "        \n",
    "        # sort by box probabilities which is index 2\n",
    "        detections.sort(key=lambda x: x[2], reverse=True)\n",
    "        TP = torch.zeros((len(detections)))\n",
    "        FP = torch.zeros((len(detections)))\n",
    "        total_true_bboxes = len(ground_truths)\n",
    "       \n",
    "        #print(f'Total true bboxes of class {c}: {total_true_bboxes}')\n",
    "        \n",
    "        # If none exists for this class then we can safely skip\n",
    "        # Maybe removing this is enough to take into account False Positives\n",
    "        # for images with no objects\n",
    "        if total_true_bboxes == 0:\n",
    "            continue\n",
    "\n",
    "        for detection_idx, detection in enumerate(detections):\n",
    "            # Only take out the ground_truths that have the same\n",
    "            # training idx as detection\n",
    "            ground_truth_img = [\n",
    "                bbox for bbox in ground_truths if bbox[0] == detection[0]\n",
    "            ]\n",
    "\n",
    "            num_gts = len(ground_truth_img)\n",
    "            best_iou = 0\n",
    "\n",
    "            for idx, gt in enumerate(ground_truth_img):\n",
    "                iou = iou_tensor(\n",
    "                    torch.tensor(detection[3:]),\n",
    "                    torch.tensor(gt[3:]),\n",
    "                    box_format=box_format,\n",
    "                )\n",
    "                # iou, _, _ = ut.iou(detection[3:], \n",
    "                #                    gt[3:]) \n",
    "\n",
    "                if iou > best_iou:\n",
    "                    best_iou = iou\n",
    "                    best_gt_idx = idx\n",
    "\n",
    "            if best_iou > iou_threshold:\n",
    "                # only detect ground truth detection once\n",
    "                if amount_bboxes[detection[0]][best_gt_idx] == 0:\n",
    "                    # true positive and add this bounding box to seen\n",
    "                    TP[detection_idx] = 1\n",
    "                    amount_bboxes[detection[0]][best_gt_idx] = 1\n",
    "                else:\n",
    "                    FP[detection_idx] = 1\n",
    "\n",
    "            # if IOU is lower then the detection is a false positive\n",
    "            else:\n",
    "                FP[detection_idx] = 1\n",
    "\n",
    "        #print(f'True Positives class {c}: {TP}')\n",
    "        #print(f'False Positives class {c}: {FP}')\n",
    "\n",
    "        TP_cumsum = torch.cumsum(TP, dim=0)\n",
    "        FP_cumsum = torch.cumsum(FP, dim=0)\n",
    "        recalls = TP_cumsum / (total_true_bboxes + epsilon)\n",
    "        precisions = TP_cumsum / (TP_cumsum + FP_cumsum + epsilon)\n",
    "\n",
    "        if precisions.numel() > 0:\n",
    "            #cls_prec.update({c: precisions[-1].numpy()})\n",
    "            cls_prec.update({c: precisions[-1].item()})\n",
    "        else:\n",
    "            cls_prec.update({c: 0.})\n",
    "        if recalls.numel() > 0:\n",
    "            #cls_rec.update({c: recalls[-1].numpy()})\n",
    "            cls_rec.update({c: recalls[-1].item()})\n",
    "        else:\n",
    "            cls_rec.update({c: 0.})\n",
    "\n",
    "        precisions = torch.cat((torch.tensor([1]), precisions))\n",
    "        recalls = torch.cat((torch.tensor([0]), recalls))\n",
    "        # torch.trapz for numerical integration\n",
    "        average_precisions.append(torch.trapz(precisions, recalls))\n",
    "        avg_prec.update({c: torch.trapz(precisions, recalls)})\n",
    "\n",
    "    mAP = sum(average_precisions) / (len(average_precisions) + epsilon)\n",
    "\n",
    "    #return mAP, average_precisions, cls_prec, cls_rec\n",
    "    return (mAP, \n",
    "            avg_prec,\n",
    "            cls_prec, \n",
    "            cls_rec)\n",
    "\n",
    "\n",
    "def get_bboxes(\n",
    "    loader,\n",
    "    model,\n",
    "    SX,\n",
    "    SY,\n",
    "    B,\n",
    "    C,\n",
    "    mask,\n",
    "    iou_threshold,\n",
    "    threshold,\n",
    "    device,\n",
    "    box_format=\"midpoint\"):\n",
    "    \n",
    "    \n",
    "    all_pred_boxes = []\n",
    "    all_true_boxes = []\n",
    "\n",
    "    # make sure model is in eval before get bboxes\n",
    "    model.eval()\n",
    "    train_idx = 0\n",
    "\n",
    "    #for batch_idx, (imgs, labels, _, _) in enumerate(loader):\n",
    "    for batch_idx, (imgs, labels) in enumerate(loader):\n",
    "        imgs = imgs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            predictions = model(imgs)\n",
    "\n",
    "        batch_size = imgs.shape[0]\n",
    "        true_bboxes = outcell_2_outboxes(out_cells=labels, \n",
    "                                         SX=SX, SY=SY, B=B, C=C, \n",
    "                                         mask=mask, \n",
    "                                         device='cpu', # Changed to cpu\n",
    "                                         is_pred=False)\n",
    "        bboxes = outcell_2_outboxes(out_cells=predictions, \n",
    "                                    SX=SX, SY=SY, B=B, C=C, \n",
    "                                    mask=mask, \n",
    "                                    device='cpu', # Changed to cpu\n",
    "                                    is_pred=True)\n",
    "\n",
    "        for idx in range(batch_size):\n",
    "            #nms_boxes = nms_yv1(\n",
    "            nms_boxes = nms_yv1_getBBoxes(\n",
    "                bboxes[idx],\n",
    "                iou_threshold=iou_threshold,\n",
    "                threshold=threshold,\n",
    "                box_format=box_format, # Midpoint, to use iou_tensor inside\n",
    "            )\n",
    "\n",
    "\n",
    "            #if batch_idx == 0 and idx == 0:\n",
    "            #    plot_image(x[idx].permute(1,2,0).to(\"cpu\"), nms_boxes)\n",
    "            #    print(nms_boxes)\n",
    "\n",
    "            for nms_box in nms_boxes:\n",
    "                all_pred_boxes.append([train_idx] + nms_box)\n",
    "\n",
    "            for box in true_bboxes[idx]:\n",
    "                # many will get converted to 0 pred, as bboxes have Conf = 1 and the rest are 0\n",
    "                if box[1] > threshold:\n",
    "                    all_true_boxes.append([train_idx] + box)\n",
    "\n",
    "            train_idx += 1\n",
    "\n",
    "    model.train()\n",
    "    return all_pred_boxes, all_true_boxes\n",
    "\n",
    "\n",
    "def outcell_2_outboxes(out_cells, SX, SY, B, C, mask, device, is_pred = True):\n",
    "    '''\n",
    "    Convert batch of cells to batch of boxes: out_cells must be of shape (BATCH_SIZE, SX, SY, B*5+C) \n",
    "        [xcel, ycel, w, h, conf, class_0, class_1] -> [pred_class, score, xc, yc, w, h]\n",
    "    \n",
    "    Arguments:\n",
    "        - out_cells: labels at loader output or predictions at model output\n",
    "            Format: [xcel, ycel, w, h, conf, class_0, class_1]\n",
    "    Return:\n",
    "        - all_bboxes: list of bounding boxes\n",
    "            Format: [[bboxes idx 0], [bboxes idx 1], ... [bboxes idx BATCH_SIZE-1]]\n",
    "    '''\n",
    "\n",
    "    out_cells = out_cells.to(device) # TAKE A LOOK TO CPU DECISION\n",
    "\n",
    "    out_cells[out_cells <= 0] = 0 # Zeroing all negative values. Avoid (-conf * -class_id) = +score\n",
    "    #out_cells[out_cells >= 1] = 1 # Clamping all values to 1. Avoid being out of the image. Maybe afterwards, although it does not hurt here.\n",
    "                                   # This is due to yolo2pixel transformation, which could led to out of image values, depending on (w, h)\n",
    "\n",
    "    batch_size = out_cells.shape[0]\n",
    "    if is_pred:\n",
    "        out_cells = out_cells.reshape(batch_size, SY, SX, B*5+C)\n",
    "    else:\n",
    "        out_cells = out_cells.reshape(batch_size, SY, SX, 5+C)\n",
    "    \n",
    "    # With 2 BBoxes, choose the one with highest confidence. How highest IOU, without label?\n",
    "    if (B>1 and is_pred):\n",
    "        '''\n",
    "        2 BB: [xcell, ycell, w, h, confidence_A, xcell, ycell, w, h, confidence_B, class_0, class_1]\n",
    "        '''\n",
    "        bbox1 = out_cells[..., :4]\n",
    "        bbox2 = out_cells[..., 5:9]\n",
    "        score = torch.cat((out_cells[..., 4:5],out_cells[..., 9:10]), dim=-1)\n",
    "        best_score, idx = torch.max(score, dim=-1, keepdim=True)\n",
    "\n",
    "        bestbbox = (1-idx)*bbox1 + idx*bbox2\n",
    "\n",
    "        #class_prob = a[..., 10:12] # Esto no hace falta, se hace debajo\n",
    "        #cls_pred = class_prob.argmax(dim=-1, keepdim=True)\n",
    "        out_cells = torch.cat((bestbbox, best_score, out_cells[..., 10:12]), dim=-1)\n",
    "        \n",
    "    \n",
    "    # All cells are converted to boxes. Format will be [xc, yc, w, h, conf, class_0, class_1]\n",
    "    boxes = cell2boxes(cells = out_cells, mask = mask)\n",
    "\n",
    "    # ================================================================================================================== #\n",
    "    #                                                                                                                    #\n",
    "    #     Convert [xc, yc, w, h, conf, class_0, class_1] to [pred_class, score, xc, yc, w, h]                            #\n",
    "    #         Identify class predicted: class_0 > class_1 ??? or the opposite                                            #\n",
    "    #         Multiply (conf * class_id) to get score and compare to threshold afterwards. It will be 1 for ground truth #\n",
    "    #                                                                                                                    #\n",
    "    # ================================================================================================================== #\n",
    "    classes = boxes[...,5:7].argmax(-1).unsqueeze(-1) # Indices of class predictes, matching class_id: index 0 -> smoke, class id = 0 // same for fire\n",
    "    #print(f'Tensor of classes predicted\\n {classes}')   \n",
    "    \n",
    "    # If SOFTMAX is used, there is no need to multiply conf * class_prob\n",
    "    # scores = ( boxes[...,4].unsqueeze(-1) ) * boxes[...,5:7] # score = confidence * [class_0_prob, class_1_prob]\n",
    "    # scores, _ = torch.max(scores, dim=-1, keepdim=True) # Get maximum values -> score of class predicted\n",
    "    scores = boxes[..., 4:5]\n",
    "    #print(f'Scores together\\n {scores}')     \n",
    "    \n",
    "    out_boxes = torch.concat((classes, scores, boxes[...,:4]), dim=-1) # Concat all data\n",
    "    #print(f'Final Output {out_boxes}')    \n",
    "\n",
    "    # =========================================== #\n",
    "    #                                             #\n",
    "    #             Convert boxes to List           #\n",
    "    #                                             #\n",
    "    #    [[bboxes idx 0] , [bboxes idx 1], etc]   #\n",
    "    #                                             #\n",
    "    # =========================================== #\n",
    "    all_bboxes = []\n",
    "\n",
    "    for ex_idx in range(batch_size):\n",
    "        bboxes = []\n",
    "\n",
    "#         for bbox_i in range(SX):\n",
    "#             for bbox_j in range(SY):\n",
    "#                 bboxes.append([x.item() for x in out_boxes[ex_idx, bbox_i, bbox_j, :]])\n",
    "        for bbox_i in range(SX):\n",
    "            for bbox_j in range(SY):\n",
    "                bboxes.append([x.item() for x in out_boxes[ex_idx, bbox_j, bbox_i, :]])     \n",
    "        all_bboxes.append(bboxes)\n",
    "    \n",
    "    return all_bboxes\n",
    "\n",
    "\n",
    "def cell2boxes(cells, mask):\n",
    "    '''\n",
    "    Converts cells to boxes using the cell2box_mask and broadcasting over batches\n",
    "    In targets, sum only when score = 1\n",
    "\n",
    "    Arguments:\n",
    "        - cells: cells to convert, as yield by Dataloader in batches\n",
    "        - mask: mask used for conversion\n",
    "\n",
    "    Return:\n",
    "        - cells converted to boxes\n",
    "    '''\n",
    "\n",
    "    #out_boxes = cells.clone().detach()\n",
    "    out_boxes = cells.detach().clone()\n",
    "    out_boxes[...,0:1] = (out_boxes[...,0:1] + mask[...,0:1])/SX \n",
    "    out_boxes[...,1:2] = (out_boxes[...,1:2] + mask[...,1:2])/SY \n",
    "\n",
    "    return out_boxes\n",
    "\n",
    "\n",
    "def plot_preds(ori_img, img_w, img_h, nms_preds, names, colors):\n",
    "    '''\n",
    "    It draws the bounding boxes over the image.\n",
    "\n",
    "    Arguments:\n",
    "        - ori_img: original image with no modification or letterbox\n",
    "        - nms_preds: Non Maximum Supression predictions [x0, y0, x1, y1, class_id, score]\n",
    "        - names: list of class names\n",
    "        - colors: list of colors asigned to each class in cv2 format (B,G,R)\n",
    "        - ratio: ratio of letterbox conversion\n",
    "        - dwdh: paddings of letterbox conversion\n",
    "\n",
    "    Returns:\n",
    "        - pic: picture with bounding boxes on top of original picture\n",
    "    '''\n",
    "    \n",
    "    pic = ori_img.copy()\n",
    "    \n",
    "    for i,(xc,yc,w,h,score,class_id) in enumerate(nms_preds):\n",
    "        box = np.array(yolo2pixel([xc,yc,w,h]))\n",
    "        box[0] = box[0]*img_w\n",
    "        box[1] = box[1]*img_h\n",
    "        box[2] = box[2]*img_w\n",
    "        box[3] = box[3]*img_h\n",
    "        box = box.round().astype(np.int32).tolist()\n",
    "        cls_id = int(class_id)\n",
    "        score = round(float(score),3)\n",
    "        name = names[cls_id]\n",
    "        color = colors[name]\n",
    "        name += ' '+str(score)\n",
    "        cv2.rectangle(pic,box[:2],box[2:],color,2) # 1 -> rectangle thickness\n",
    "        cv2.putText(pic,name,(box[0]+6, box[1] + 20),cv2.FONT_HERSHEY_SIMPLEX,0.4,[225, 255, 255],thickness=1)  # 0.5 -> font size\n",
    "\n",
    "    return pic\n",
    "\n",
    "def save_log(epochs, \n",
    "             train_total_loss,\n",
    "             train_box_loss,\n",
    "             train_class_loss,\n",
    "             train_confidence_loss,\n",
    "             train_noobj_loss,\n",
    "             train_mAP,\n",
    "             train_class_AP,\n",
    "             train_class_precision,\n",
    "             train_class_recall,\n",
    "             val_total_loss,\n",
    "             val_box_loss,\n",
    "             val_class_loss,\n",
    "             val_confidence_loss,\n",
    "             val_noobj_loss,\n",
    "             val_mAP,\n",
    "             val_class_AP,\n",
    "             val_class_precision,\n",
    "             val_class_recall,\n",
    "             log_file_dst):\n",
    "    \n",
    "    '''\n",
    "    Create a dictionary with all metrics\n",
    "    Save the dictionary as excel file with Pandas\n",
    "    '''\n",
    "    \n",
    "    epoch_range = range(epochs)\n",
    "    log_file = {}\n",
    "    log_file.update({\"epoch\": epoch_range})\n",
    "    \n",
    "    # Train Losses\n",
    "    log_file.update({\"train_total_loss\": train_total_loss})\n",
    "    log_file.update({\"train_box_loss\": train_box_loss})\n",
    "    log_file.update({\"train_class_loss\": train_class_loss})\n",
    "    log_file.update({\"train_confidence_loss\": train_confidence_loss})\n",
    "    log_file.update({\"train_noobj_loss\": train_noobj_loss})\n",
    "    \n",
    "    # Train mAP, Class AP, Precision, Recall\n",
    "    train_mAP_log = []\n",
    "    for e in train_mAP:\n",
    "        #train_mAP_log.append(e.numpy())\n",
    "        #train_mAP_log.append(e)\n",
    "        train_mAP_log.append(e.item())\n",
    "    log_file.update({\"train_mAP\": train_mAP_log})\n",
    "    \n",
    "    train_smk_AP = []\n",
    "    train_fire_AP = []\n",
    "    for e in train_class_AP:\n",
    "        #train_smk_AP.append(e[0].numpy())\n",
    "        #train_smk_AP.append(e[0])\n",
    "        train_smk_AP.append(e[0].item())\n",
    "        #train_fire_AP.append(e[1].numpy())\n",
    "        #train_fire_AP.append(e[1])\n",
    "        train_fire_AP.append(e[1].item())\n",
    "    log_file.update({\"train_smk_AP\": train_smk_AP})\n",
    "    log_file.update({\"train_fire_AP\": train_fire_AP})\n",
    "    \n",
    "    \n",
    "    train_smk_precision = []\n",
    "    train_fire_precision = []\n",
    "    for e in train_class_precision:\n",
    "        #train_smk_precision.append(e[0].numpy())\n",
    "        train_smk_precision.append(e[0])\n",
    "        #train_fire_precision.append(e[1].numpy())\n",
    "        train_fire_precision.append(e[1])\n",
    "    log_file.update({\"train_smk_precision\": train_smk_precision})\n",
    "    log_file.update({\"train_fire_precision\": train_fire_precision})\n",
    "        \n",
    "    train_smk_recall = []\n",
    "    train_fire_recall = []\n",
    "    for e in train_class_recall:\n",
    "        #train_smk_recall.append(e[0].numpy())\n",
    "        train_smk_recall.append(e[0])\n",
    "        #train_fire_recall.append(e[1].numpy())\n",
    "        train_fire_recall.append(e[1])\n",
    "    log_file.update({\"train_smk_recall\": train_smk_recall})\n",
    "    log_file.update({\"train_fire_recall\": train_fire_recall})\n",
    "    \n",
    "    # Validation Losses\n",
    "    log_file.update({\"val_total_loss\": val_total_loss})\n",
    "    log_file.update({\"val_box_loss\": val_box_loss})\n",
    "    log_file.update({\"val_class_loss\": val_class_loss})\n",
    "    log_file.update({\"val_confidence_loss\": val_confidence_loss})\n",
    "    log_file.update({\"val_noobj_loss\": val_noobj_loss})\n",
    "    \n",
    "    # Val mAP, Class AP, Precision, Recall\n",
    "    val_mAP_log = []\n",
    "    for e in val_mAP:\n",
    "        #val_mAP_log.append(e.numpy())\n",
    "        val_mAP_log.append(e.item())\n",
    "    log_file.update({\"val_mAP\": val_mAP_log})\n",
    "    \n",
    "    val_smk_AP = []\n",
    "    val_fire_AP = []\n",
    "    for e in val_class_AP:\n",
    "        val_smk_AP.append(e[0].item())\n",
    "        val_fire_AP.append(e[1].item())\n",
    "    log_file.update({\"val_smk_AP\": val_smk_AP})\n",
    "    log_file.update({\"val_fire_AP\": val_fire_AP})\n",
    "    \n",
    "    val_smk_precision = []\n",
    "    val_fire_precision = []\n",
    "    for e in val_class_precision:\n",
    "        val_smk_precision.append(e[0])\n",
    "        val_fire_precision.append(e[1])\n",
    "    log_file.update({\"val_smk_precision\": val_smk_precision})\n",
    "    log_file.update({\"val_fire_precision\": val_fire_precision})\n",
    "    \n",
    "    val_smk_recall = []\n",
    "    val_fire_recall = []\n",
    "    for e in val_class_recall:\n",
    "        val_smk_recall.append(e[0])\n",
    "        val_fire_recall.append(e[1])\n",
    "    log_file.update({\"val_smk_recall\": val_smk_recall})\n",
    "    log_file.update({\"val_fire_recall\": val_fire_recall})\n",
    "    \n",
    "    df = pd.DataFrame(log_file)\n",
    "    df.to_excel(log_file_dst)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5891d5e-3ee7-45b3-ad2a-fe9d6c1ff00b",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76e6703-39c8-41d1-940a-91c1de8c8809",
   "metadata": {},
   "source": [
    "# DFire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c57d78ca-ac32-41b6-bff4-991135f5e9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DFireDataset(Dataset):\n",
    "    '''\n",
    "    Creates a Pytorch Dataset to train the Yolov1 Network.\n",
    "    Encodes labels to match the format [xcell, ycell, w, h, confidence, class_0 (smoke), class_1 (fire)]\n",
    "        - Final encoding format is: [xcell, ycell, w, h, conf=1, smoke?, fire?]\n",
    "\n",
    "    Discard images when there are more than 1 object in the same cell\n",
    "    \n",
    "    Arguments:\n",
    "        - img_h:            image height\n",
    "        - img_w:            image width\n",
    "        - img_dir:          path to images folder\n",
    "        - label_dir:        path to labels folder\n",
    "        - SX:               number of cells in X axis (horizontal -> width)\n",
    "        - SY:               number of cells in Y axis (vertical -> height)\n",
    "        - C:                number of classes, 2 in this case\n",
    "        - transform:        transformation applied to input images -> Albumentations\n",
    "        - target_transform: transformation applied to labels -> nothing by default\n",
    "\n",
    "    Return:\n",
    "        - img:              1 image of the dataset\n",
    "        - target:           corresponding label encoded\n",
    "    '''\n",
    "\n",
    "    def __init__(self, img_h, img_w, img_dir, label_dir, \n",
    "                 SX, SY, C, \n",
    "                 transform=None, target_transform=None):\n",
    "        self.img_h = img_h\n",
    "        self.img_w = img_w\n",
    "        self.img_dir = img_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.SX = SX\n",
    "        self.SY = SY\n",
    "        self.C = C\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "        self.labels_list = sorted(\n",
    "            [\n",
    "                os.path.join(self.label_dir, file_name)\n",
    "                for file_name in os.listdir(self.label_dir)\n",
    "                if file_name.endswith(\".txt\")\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        self.images, self.bboxes, self.labels = self.__build_ds__(self.labels_list)\n",
    "        \n",
    "        self.num_samples = self.images.shape[0]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "    \n",
    "    def __bbox_check__(self, bbox):\n",
    "        eps = 1e-6\n",
    "        \n",
    "        xc, yc, w, h = bbox[0], bbox[1], bbox[2], bbox[3]\n",
    "        xmin = xc - w/2\n",
    "        ymin = yc - h/2\n",
    "        xmax = xc + w/2\n",
    "        ymax = yc + h/2\n",
    "        \n",
    "        xmin = max(xmin, 0 + eps)\n",
    "        ymin = max(ymin, 0 + eps)\n",
    "        xmax = min(xmax, 1)\n",
    "        ymax = min(ymax, 1)\n",
    "        \n",
    "        bbox = np.array([ \n",
    "                (xmin+xmax)/2,\n",
    "                (ymin+ymax)/2,\n",
    "                xmax-xmin,\n",
    "                ymax-ymin\n",
    "                 ]).astype(np.float32)\n",
    "        \n",
    "        return bbox        \n",
    "\n",
    "\n",
    "    def __build_ds__(self, labels_list):\n",
    "        bboxes = []\n",
    "        labels = []\n",
    "        images = []\n",
    "        wrong_imgs = 0\n",
    "        overlapping_rem = 0\n",
    "        more_than_3 = 0\n",
    "                \n",
    "        for label in labels_list:\n",
    "            fname = Path(label).stem\n",
    "            image_path = self.img_dir + fname + '.jpg'   \n",
    "            #print(fname, image_path)\n",
    "                                   \n",
    "            if cv2.imread(image_path) is None:\n",
    "                print(f'{image_path} cannot be read by cv2 -> removed')\n",
    "                wrong_imgs += 1\n",
    "            \n",
    "            else:\n",
    "                \n",
    "                label_mtx = np.zeros((self.SY, self.SX))\n",
    "                overlapping_object = 0\n",
    "\n",
    "                one_bboxes = []\n",
    "                one_labels = []\n",
    "            \n",
    "                with open(label) as f:\n",
    "                    lines = f.readlines()\n",
    "\n",
    "                    # Restrict to 3 boxes per sample\n",
    "                    if len(lines) > 3:\n",
    "                        more_than_3 += 1\n",
    "                        continue\n",
    "                        \n",
    "                    for line in lines:\n",
    "                        class_id, x, y, w, h = line.strip().split()\n",
    "                        class_id = int(class_id)\n",
    "                        box = np.array([x, y, w, h]).astype(np.float32)\n",
    "                        x, y, w, h = box[0], box[1], box[2], box[3]\n",
    "                        box_ok = self.__bbox_check__([x, y, w, h])\n",
    "                        x, y, w, h = box_ok[0], box_ok[1], box_ok[2], box_ok[3]\n",
    "                        i, j = math.floor(y * self.SY), math.floor(x * self.SX)\n",
    "                        if label_mtx[i, j] == 1:\n",
    "                            overlapping_object = 1\n",
    "                            overlapping_rem += 1\n",
    "                            #print(f'Removed {label} due to overlapping object in cell {i, j}')\n",
    "                            break\n",
    "                        else:\n",
    "                            label_mtx[i, j] = 1\n",
    "                            one_bboxes.append([x, y, w, h])\n",
    "                            # smoke\n",
    "                            if class_id == 0:\n",
    "                                one_labels.append(0)\n",
    "                            # fire\n",
    "                            elif class_id == 1:\n",
    "                                one_labels.append(1)\n",
    "                            else:\n",
    "                                print(f'File {label} errored in cell {i, j}')\n",
    "\n",
    "                    if overlapping_object == 0:\n",
    "                        # Padding to SX*SY labels and bounding boxes, so you can store tensors\n",
    "                        # Label -1 indicates no box\n",
    "                        for idx in range(self.SX*self.SY - len(one_labels)):\n",
    "                            one_bboxes.append([0, 0, 0, 0])\n",
    "                            one_labels.append(-1)\n",
    "                        # print(f'\\nBboxes and Labels of image {image_path}')\n",
    "                        # print(\"Bboxes\")\n",
    "                        # for box in one_bboxes:\n",
    "                        #     print(box)\n",
    "                        # print(\"Labels\")\n",
    "                        # for label in one_labels:\n",
    "                        #     print(label)\n",
    "                        bboxes.append(one_bboxes)\n",
    "                        labels.append(one_labels)\n",
    "                        images.append(image_path)\n",
    "        \n",
    "        print(f'Removed wrong images: {wrong_imgs}')\n",
    "        print(f'Removed due to overlapping: {overlapping_rem}')\n",
    "        print(f'Removed due to more than 3: {more_than_3}')\n",
    "        logger.info(f'Removed wrong images: {wrong_imgs}')\n",
    "        logger.info(f'Removed due to overlapping: {overlapping_rem}')\n",
    "        logger.info(f'Removed due to more than 3: {more_than_3}')\n",
    "\n",
    "        labels_np = np.array(labels)\n",
    "        labels_tensor = torch.tensor(labels_np, dtype=torch.float32)\n",
    "        bboxes_np = np.array(bboxes)\n",
    "        bboxes_tensor = torch.tensor(bboxes_np, dtype=torch.float32)\n",
    "        images_array = np.array(images)\n",
    "        # print(f'Images array {images_array}')\n",
    "        # print(f'Bboxes tensor {bboxes_tensor}')\n",
    "        # print(f'Labels tensor {labels_tensor}')\n",
    "        \n",
    "        return images_array, bboxes_tensor, labels_tensor\n",
    "        #return images, bboxes, labels\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        # Image processing\n",
    "        img_file = self.images[index]\n",
    "        img = cv2.imread(img_file)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)   \n",
    "        #img = cv2.resize(img, (self.img_w, self.img_h), interpolation = cv2.INTER_NEAREST)\n",
    "\n",
    "        # Labels processing\n",
    "        bboxes = self.bboxes[index]\n",
    "        bboxes = bboxes[~torch.all(bboxes == torch.tensor([0,0,0,0]), dim=1)]\n",
    "        bboxes = bboxes.numpy().tolist()\n",
    "        #print(bboxes)\n",
    "        labels = self.labels[index]\n",
    "        labels = labels[labels != -1.]\n",
    "        labels = labels.numpy().tolist()\n",
    "        #print(f'Labels inside dataset {labels}')\n",
    "        \n",
    "        # Data Augmentation\n",
    "        if self.transform is not None:\n",
    "            try:\n",
    "                aug = self.transform(image=img, bboxes=bboxes, class_labels=labels)\n",
    "                img = aug['image']\n",
    "                bboxes = aug['bboxes']\n",
    "                labels = aug['class_labels']\n",
    "            except:\n",
    "                #print(f'Error trying to augment image {img_file}')\n",
    "                img = cv2.resize(img, (self.img_w, self.img_h), interpolation = cv2.INTER_NEAREST)\n",
    "                img = (img / 255.) - 0.5\n",
    "                img = torch.tensor(img, dtype=torch.float32)\n",
    "                img = img.permute(2, 0, 1)\n",
    "        \n",
    "        label_mtx = np.zeros((self.SY, self.SX, 5+self.C))\n",
    "        \n",
    "        for box, label in zip(bboxes, labels):\n",
    "            class_id = int(label)\n",
    "            i, j = int(box[1]*self.SY), int(box[0]*self.SX)\n",
    "            xcell, ycell = box[0]*self.SX - j, box[1]*self.SY - i\n",
    "            label_mtx[i, j, :5] = [xcell, ycell, box[2], box[3], 1]\n",
    "            label_mtx[i, j, 5+class_id] = 1\n",
    "\n",
    "        label_mtx = torch.tensor(label_mtx, dtype=torch.float32)\n",
    "        \n",
    "        #return img, label_mtx, img_file\n",
    "        return img, label_mtx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee1de61-d47b-470c-8521-26823fee52f0",
   "metadata": {},
   "source": [
    "# DFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "79dac75c-3e82-4be0-acde-dea4e7007e86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DFS number of samples: 9462\n",
      "DFS train samples: 7569\n",
      "DFS test samples: 1893\n"
     ]
    }
   ],
   "source": [
    "dfs_base_dir = 'ds2fire/dfs_xml/'\n",
    "dfs_images_dir = dfs_base_dir + 'images/'\n",
    "dfs_labels_dir = dfs_base_dir + 'labels/'\n",
    "\n",
    "\n",
    "# Get all txt file paths in path_annot and sort them\n",
    "dfs_xml_files = sorted(\n",
    "    [\n",
    "        os.path.join(dfs_labels_dir, file_name)\n",
    "        for file_name in os.listdir(dfs_labels_dir)\n",
    "        if file_name.endswith(\".xml\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "dfs_len = len(dfs_xml_files)\n",
    "dfs_train_elements = int(dfs_len*0.8)\n",
    "dfs_test_elements = dfs_len - dfs_train_elements\n",
    "\n",
    "random.shuffle(dfs_xml_files)\n",
    "dfs_train_list = dfs_xml_files[:dfs_train_elements]\n",
    "dfs_test_list = dfs_xml_files[dfs_train_elements:]\n",
    "\n",
    "print(f'DFS number of samples: {dfs_len}')\n",
    "print(f'DFS train samples: {len(dfs_train_list)}')\n",
    "print(f'DFS test samples: {len(dfs_test_list)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5564261d-60af-404b-9bda-ab4af2f44448",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DFSDataset(Dataset):\n",
    "    '''\n",
    "    Creates a Pytorch Dataset to train the Yolov1 Network.\n",
    "    Encodes labels to match the format [xcell, ycell, w, h, confidence, class_0 (smoke), class_1 (fire)]\n",
    "        - Final encoding format is: [xcell, ycell, w, h, conf=1, smoke?, fire?]\n",
    "\n",
    "    Discard images when there are more than 1 object in the same cell\n",
    "    \n",
    "    Arguments:\n",
    "        - img_h:            image height\n",
    "        - img_w:            image width\n",
    "        - img_dir:          path to images folder\n",
    "        - label_dir:        path to labels folder\n",
    "        - SX:               number of cells in X axis (horizontal -> width)\n",
    "        - SY:               number of cells in Y axis (vertical -> height)\n",
    "        - C:                number of classes, 2 in this case\n",
    "        - transform:        transformation applied to input images -> Albumentations\n",
    "        - target_transform: transformation applied to labels -> nothing by default\n",
    "\n",
    "    Return:\n",
    "        - img:              1 image of the dataset\n",
    "        - target:           corresponding label encoded\n",
    "    '''\n",
    "\n",
    "    def __init__(self, img_h, img_w, img_dir, labels_list, \n",
    "                 SX, SY, C, \n",
    "                 transform=None, target_transform=None):\n",
    "        self.img_h = img_h\n",
    "        self.img_w = img_w\n",
    "        self.img_dir = img_dir\n",
    "        self.labels_list = labels_list\n",
    "        self.SX = SX\n",
    "        self.SY = SY\n",
    "        self.C = C\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        \n",
    "        self.images, self.bboxes, self.labels = self.__build_ds__(self.labels_list)\n",
    "        \n",
    "        self.num_samples = self.images.shape[0]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "    \n",
    "    def __bbox_check__(self, bbox):\n",
    "        eps = 1e-6\n",
    "        \n",
    "        xmin, ymin, xmax, ymax = bbox[0], bbox[1], bbox[2], bbox[3]\n",
    "        \n",
    "        xmin = max(xmin, 0 + eps)\n",
    "        ymin = max(ymin, 0 + eps)\n",
    "        xmax = min(xmax, 1)\n",
    "        ymax = min(ymax, 1)\n",
    "        \n",
    "        bbox = np.array([ \n",
    "                (xmin+xmax)/2,\n",
    "                (ymin+ymax)/2,\n",
    "                xmax-xmin,\n",
    "                ymax-ymin\n",
    "                 ]).astype(np.float32)\n",
    "        \n",
    "        return bbox        \n",
    "\n",
    "\n",
    "    def __build_ds__(self, labels_list):\n",
    "        bboxes = []\n",
    "        labels = []\n",
    "        images = []\n",
    "        wrong_imgs = 0\n",
    "        overlapping_rem = 0\n",
    "        more_than_3 = 0\n",
    "        \n",
    "        \n",
    "        for xml_file in labels_list:\n",
    "#             fname = Path(xml_file).stem\n",
    "#             image_path = self.img_dir + fname + '.jpg'   \n",
    "            #print(fname, image_path)\n",
    "            \n",
    "            tree = ET.parse(xml_file)\n",
    "            root = tree.getroot()\n",
    "\n",
    "            image_name = root.find(\"filename\").text\n",
    "            image_path = os.path.join(self.img_dir, image_name)\n",
    "            #print(image_name, image_path)\n",
    "            \n",
    "            if cv2.imread(image_path) is None:\n",
    "                print(f'{image_path} cannot be read by cv2 -> removed')\n",
    "                wrong_imgs += 1\n",
    "            \n",
    "            else:\n",
    "                overlapping_object = 0\n",
    "                more_than_3_in = 0\n",
    "\n",
    "                label_mtx = np.zeros((self.SY, self.SX))\n",
    "\n",
    "                size = root.find(\"size\")\n",
    "                img_w = float(size.find(\"width\").text)\n",
    "                img_h = float(size.find(\"height\").text)\n",
    "\n",
    "                one_bboxes = []\n",
    "                one_labels = []\n",
    "\n",
    "                for obj in root.iter(\"object\"):\n",
    "                    class_name = obj.find(\"name\").text\n",
    "                    if class_name == 'smoke':\n",
    "                        class_id = 0\n",
    "                        more_than_3_in += 1\n",
    "                    elif class_name == 'fire':\n",
    "                        class_id = 1\n",
    "                        more_than_3_in += 1\n",
    "                    else:\n",
    "                        continue \n",
    "                    bbox = obj.find(\"bndbox\")\n",
    "                    xmin = float(bbox.find(\"xmin\").text)/img_w\n",
    "                    ymin = float(bbox.find(\"ymin\").text)/img_h\n",
    "                    xmax = float(bbox.find(\"xmax\").text)/img_w\n",
    "                    ymax = float(bbox.find(\"ymax\").text)/img_h\n",
    "\n",
    "                    box = self.__bbox_check__([xmin, ymin, xmax, ymax])\n",
    "                    #print(f'Class: {class_name} - Class_id: {class_id}. Coords: x={x}, y={y}, w={w}, h={h}')\n",
    "\n",
    "                    x, y, w, h = box[0], box[1], box[2], box[3]\n",
    "                    i, j = math.floor(y * self.SY), math.floor(x * self.SX)\n",
    "                    if label_mtx[i, j] == 1:\n",
    "                        overlapping_object = 1\n",
    "                        overlapping_rem += 1\n",
    "                        break\n",
    "                    else:\n",
    "                        label_mtx[i, j] = 1\n",
    "                        one_bboxes.append([x, y, w, h])\n",
    "                        # smoke\n",
    "                        if class_id == 0:\n",
    "                            one_labels.append(0)\n",
    "                        # fire\n",
    "                        elif class_id == 1:\n",
    "                            one_labels.append(1)\n",
    "                        else:\n",
    "                            print(f'File {label} errored in cell {i, j}') \n",
    "                \n",
    "                if more_than_3_in > 3:\n",
    "                    more_than_3 += 1\n",
    "                    continue\n",
    "                \n",
    "                if (overlapping_object == 0) and (more_than_3_in < 4):\n",
    "                    # Padding to SX*SY labels and bounding boxes, so you can store tensors\n",
    "                    # Label -1 indicates no box\n",
    "                    for idx in range(self.SX*self.SY - len(one_labels)):\n",
    "                        one_bboxes.append([0, 0, 0, 0])\n",
    "                        one_labels.append(-1)\n",
    "                    # print(f'\\nBboxes and Labels of image {image_path}')\n",
    "                    # print(\"Bboxes\")\n",
    "                    # for box in one_bboxes:\n",
    "                    #     print(box)\n",
    "                    # print(\"Labels\")\n",
    "                    # for label in one_labels:\n",
    "                    #     print(label)\n",
    "                    bboxes.append(one_bboxes)\n",
    "                    labels.append(one_labels)\n",
    "                    images.append(image_path)\n",
    "\n",
    "        print(f'Removed wrong images: {wrong_imgs}')\n",
    "        print(f'Removed due to overlapping: {overlapping_rem}')\n",
    "        print(f'Removed due to more than 3: {more_than_3}')\n",
    "        logger.info(f'Removed wrong images: {wrong_imgs}')\n",
    "        logger.info(f'Removed due to overlapping: {overlapping_rem}')\n",
    "        logger.info(f'Removed due to more than 3: {more_than_3}')\n",
    "\n",
    "        labels_np = np.array(labels)\n",
    "        labels_tensor = torch.tensor(labels_np, dtype=torch.float32)\n",
    "        bboxes_np = np.array(bboxes)\n",
    "        bboxes_tensor = torch.tensor(bboxes_np, dtype=torch.float32)\n",
    "        images_array = np.array(images)\n",
    "        # print(f'Images array {images_array}')\n",
    "        # print(f'Bboxes tensor {bboxes_tensor}')\n",
    "        # print(f'Labels tensor {labels_tensor}')\n",
    "        \n",
    "        return images_array, bboxes_tensor, labels_tensor\n",
    "        #return images, bboxes, labels\n",
    "\n",
    "\n",
    " \n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        # Image processing\n",
    "        img_file = self.images[index]\n",
    "        img = cv2.imread(img_file)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)   \n",
    "        #img = cv2.resize(img, (self.img_w, self.img_h), interpolation = cv2.INTER_NEAREST)\n",
    "\n",
    "        # Labels processing\n",
    "        bboxes = self.bboxes[index]\n",
    "        bboxes = bboxes[~torch.all(bboxes == torch.tensor([0,0,0,0]), dim=1)]\n",
    "        bboxes = bboxes.numpy().tolist()\n",
    "        #print(bboxes)\n",
    "        labels = self.labels[index]\n",
    "        labels = labels[labels != -1.]\n",
    "        labels = labels.numpy().tolist()\n",
    "        #print(f'Labels inside dataset {labels}')\n",
    "        \n",
    "        # Data Augmentation\n",
    "        if self.transform is not None:\n",
    "            try:\n",
    "                aug = self.transform(image=img, bboxes=bboxes, class_labels=labels)\n",
    "                img = aug['image']\n",
    "                bboxes = aug['bboxes']\n",
    "                labels = aug['class_labels']\n",
    "            except:\n",
    "                #print(f'Error trying to augment image {img_file}')\n",
    "                img = cv2.resize(img, (self.img_w, self.img_h), interpolation = cv2.INTER_NEAREST)\n",
    "                img = (img / 255.) - 0.5\n",
    "                img = torch.tensor(img, dtype=torch.float32)\n",
    "                img = img.permute(2, 0, 1)\n",
    "        \n",
    "        label_mtx = np.zeros((self.SY, self.SX, 5+self.C))\n",
    "        \n",
    "        for box, label in zip(bboxes, labels):\n",
    "            class_id = int(label)\n",
    "            i, j = int(box[1]*self.SY), int(box[0]*self.SX)\n",
    "            xcell, ycell = box[0]*self.SX - j, box[1]*self.SY - i\n",
    "            label_mtx[i, j, :5] = [xcell, ycell, box[2], box[3], 1]\n",
    "            label_mtx[i, j, 5+class_id] = 1\n",
    "\n",
    "        label_mtx = torch.tensor(label_mtx, dtype=torch.float32)\n",
    "        \n",
    "        #return img, label_mtx, img_file\n",
    "        return img, label_mtx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8b488d-9139-4a9e-8af5-de827368eff1",
   "metadata": {},
   "source": [
    "# Visualize some examples with a Test Train Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d709355-0e9b-4793-bbbc-35e59c93e77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "view_dir = 'dfire4ex/train/'\n",
    "\n",
    "view_train_imgs = view_dir + 'images/'\n",
    "view_train_labels = view_dir + 'labels/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0adcae50-29e5-4ccd-a546-77bca415fdf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "VIEW_BATCH_SIZE = 16\n",
    "VIEW_NUM_WORKERS = 1\n",
    "VIEW_PIN_MEMORY = True\n",
    "\n",
    "VIEW_S = 7\n",
    "VIEW_C = 2\n",
    "VIEW_B = 1\n",
    "\n",
    "VIEW_IMG_W = 224\n",
    "VIEW_IMG_H = 224"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92020f5e-55c6-4ab9-be81-100e1dae17de",
   "metadata": {},
   "source": [
    "# Data Aug Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2186e383-3cf2-42b8-b704-7f478b099532",
   "metadata": {},
   "outputs": [],
   "source": [
    "view_transform = A.Compose([\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.RandomBrightnessContrast(p=0.5),\n",
    "    A.RandomSizedBBoxSafeCrop(height=int(1.4*VIEW_IMG_H),\n",
    "                              width= int(1.4*VIEW_IMG_W),\n",
    "                              erosion_rate=0.6,\n",
    "                              p=0.3),\n",
    "    A.HueSaturationValue(hue_shift_limit=10, p=0.2),\n",
    "    A.Blur(blur_limit=(3,3), p=0.2),\n",
    "    A.CLAHE(clip_limit=2.0, p=0.3),\n",
    "    # Shifting, scaling and rotation could dive 2 bbox inside same grid...\n",
    "    #A.ShiftScaleRotate(rotate_limit=5, p=0.2),\n",
    "    A.Resize(VIEW_IMG_H, VIEW_IMG_W, p=1),\n",
    "    #A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), p=1),\n",
    "    ToTensorV2(p=1),\n",
    "], bbox_params=A.BboxParams(format='yolo', \n",
    "                            #min_area=6*6, \n",
    "                            #min_visibility=0.05, \n",
    "                            label_fields=['class_labels']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c4ce4e-d822-4277-84c9-0998d8738dc5",
   "metadata": {},
   "source": [
    "# DFS Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "063fe2e3-a1b4-489f-96e1-8031cfbd09c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# logger.info('\\nBefore View Dataset Examples')\n",
    "\n",
    "# view_dfs_dataset = DFSDataset(img_h = VIEW_IMG_H,\n",
    "#                               img_w = VIEW_IMG_W,\n",
    "#                               img_dir = dfs_images_dir,\n",
    "#                               labels_list = dfs_test_list,\n",
    "#                               SX = VIEW_S,\n",
    "#                               SY = VIEW_S,\n",
    "#                               C = VIEW_C,\n",
    "#                               transform=view_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055a826a-f1b0-41b2-816b-fce4fed5f5b5",
   "metadata": {},
   "source": [
    "# View Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "09f1e84b-dcbe-42d1-93bd-3c37ee0d755d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# view_loader = DataLoader(dataset=view_dfs_dataset,\n",
    "#                          batch_size=VIEW_BATCH_SIZE,\n",
    "#                          num_workers=VIEW_NUM_WORKERS,\n",
    "#                          pin_memory=VIEW_PIN_MEMORY,\n",
    "#                          shuffle=True,\n",
    "#                          drop_last=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89089a0-13b7-4248-b7cb-9b7228fa62c8",
   "metadata": {},
   "source": [
    "# Utils to Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b95cbbb1-2531-482e-9ecf-f78144210e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bboxes_from_label_mtx(label_mtx):\n",
    "\n",
    "    c2b_mtx = np.zeros((VIEW_S, VIEW_S, 2))\n",
    "    for j in range(VIEW_S):\n",
    "        for i in range(VIEW_S):\n",
    "            c2b_mtx[i, j, 0] = j\n",
    "            c2b_mtx[i, j, 1] = i\n",
    "\n",
    "    label_mtx = label_mtx.numpy()\n",
    "    label_xy = label_mtx[..., :2]\n",
    "    label_rest = label_mtx[..., 2:]\n",
    "\n",
    "    c2b_xy = (c2b_mtx+label_xy)/VIEW_S\n",
    "    out = np.concatenate((c2b_xy, label_rest), axis=-1)\n",
    "    #print(f'Concat out\\n {out}')\n",
    "\n",
    "    bboxes_list = np.reshape(out, (VIEW_S*VIEW_S, 5+VIEW_C))\n",
    "\n",
    "    bboxes_list = [bbox for bbox in bboxes_list.tolist() if bbox[4]==1]\n",
    "\n",
    "    return bboxes_list\n",
    "\n",
    "\n",
    "def plot_pytorch(ds_img, img_w, img_h, nms_preds):\n",
    "    '''\n",
    "    It draws the bounding boxes over the image.\n",
    "\n",
    "    Arguments:\n",
    "        - ori_img: original image with no modification or letterbox\n",
    "        - nms_preds: Non Maximum Supression predictions [x0, y0, x1, y1, class_id, score]\n",
    "        - names: list of class names\n",
    "        - colors: list of colors asigned to each class in cv2 format (B,G,R)\n",
    "        - ratio: ratio of letterbox conversion\n",
    "        - dwdh: paddings of letterbox conversion\n",
    "\n",
    "    Returns:\n",
    "        - pic: picture with bounding boxes on top of original picture\n",
    "    '''\n",
    "\n",
    "    names ={0: 'smoke', 1: 'fire'}\n",
    "    colors = {'smoke': (0,255,255), 'fire': (255,255,0)}\n",
    "    # NEVER remove copy() or use np.ascontiguousarray()\n",
    "    pic = ds_img.numpy().copy()       \n",
    "\n",
    "    for i,(xc,yc,w,h,score,smoke,fire) in enumerate(nms_preds):\n",
    "        xmin, ymin, xmax, ymax = xc - w/2, yc - h/2, xc + w/2, yc + h/2\n",
    "        box = np.array([xmin, ymin, xmax, ymax]).astype(np.float32)\n",
    "        box[0] = box[0]*img_w\n",
    "        box[1] = box[1]*img_h\n",
    "        box[2] = box[2]*img_w-1 # avoid out of limits due to rounding\n",
    "        box[3] = box[3]*img_h-1 # avoid out of limits due to rounding\n",
    "        box = box.round().astype(np.uint8).tolist()\n",
    "        if smoke == 1:\n",
    "            cls_id = 0\n",
    "        elif fire == 1:\n",
    "            cls_id = 1\n",
    "        else:\n",
    "            print(\"Error: no valid class\")\n",
    "        score = round(float(score),3)\n",
    "        name = names[cls_id]\n",
    "        color = colors[name]\n",
    "        name += ' '+str(score)\n",
    "        cv2.rectangle(pic, box[:2], box[2:], color, 1) \n",
    "        cv2.putText(pic,name,(box[0]+6, box[1] + 20),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX,0.4,[225, 255, 255],\n",
    "                    thickness=1)  # 0.5 -> font size\n",
    "\n",
    "    return pic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31188003-ce10-432d-99ba-f8658141aa35",
   "metadata": {},
   "source": [
    "# Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "52d065b7-b186-4d68-b558-96546ee48898",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #for i, (img, label_mtx, img_path) in enumerate(view_loader):\n",
    "\n",
    "# plt.figure(figsize=(64, 32))\n",
    "\n",
    "# for i, (img, label_mtx) in enumerate(view_loader):\n",
    "#     #print(f'Batch id = {i}')\n",
    "\n",
    "#     plt.subplots(2,4)\n",
    "\n",
    "#     for idx in range(VIEW_BATCH_SIZE):\n",
    "#         #print(f'Index inside batch = {idx}')\n",
    "#         #print(img_path[idx])\n",
    "#         bboxes = get_bboxes_from_label_mtx(label_mtx[idx])\n",
    "#         #print(bboxes)\n",
    "#         img_ds = plot_pytorch(img[idx].permute(1, 2, 0), \n",
    "#                               VIEW_IMG_W, \n",
    "#                               VIEW_IMG_H, \n",
    "#                               bboxes)\n",
    "#         plt.subplot(2, 4, idx+1)\n",
    "#         plt.imshow(img_ds)\n",
    "#         #plt.imshow(img[idx].permute(1, 2, 0))\n",
    "        \n",
    "#         if (idx == 7):\n",
    "#             break\n",
    "#     #if (idx == 1):\n",
    "#     plt.tight_layout(w_pad=100)\n",
    "#     plt.show()\n",
    "#     break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c108028-a23a-410d-8af0-2853e5b85f90",
   "metadata": {},
   "source": [
    "# BED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f7209620-aa42-4946-aa62-9f645e07f926",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BED(nn.Module):\n",
    "    def __init__(self, num_classes, S, B, in_channels=3):\n",
    "        super(BED, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.num_classes = num_classes\n",
    "        self.S = S\n",
    "        self.B = B\n",
    "        \n",
    "        self.model = self.__create_BED__()\n",
    "\n",
    "        \n",
    "    def __create_BED__(self):\n",
    "        BED_model = nn.Sequential(\n",
    "            # Conv2d [in_channels, out_channels, kernel_size, stride, padding, bias]\n",
    "\n",
    "            # CNNBlock 224x224\n",
    "            nn.Conv2d(self.in_channels, 64, kernel_size=3, stride=1, padding=1,  bias=False),\n",
    "            nn.BatchNorm2d(64, affine=False),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            # CNNBlock 112x112\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(64, 24, kernel_size=3, stride=1, padding=1,  bias=False),\n",
    "            nn.BatchNorm2d(24, affine=False),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            # CNNBlock 56x56\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            # kernel = 1 in github\n",
    "            nn.Conv2d(24, 16, kernel_size=1, stride=1, padding=0,  bias=False),\n",
    "            nn.BatchNorm2d(16, affine=False),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1,  bias=False),\n",
    "            nn.BatchNorm2d(32, affine=False),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            # kernel = 1 in github\n",
    "            nn.Conv2d(32, 32, kernel_size=1, stride=1, padding=0,  bias=False),\n",
    "            nn.BatchNorm2d(32, affine=False),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1,  bias=False),\n",
    "            nn.BatchNorm2d(64, affine=False),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            # CNNBlock 28x28\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(64, 32, kernel_size=1, stride=1, padding=0,  bias=False),\n",
    "            nn.BatchNorm2d(32, affine=False),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1,  bias=False),\n",
    "            nn.BatchNorm2d(64, affine=False),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Conv2d(64, 32, kernel_size=1, stride=1, padding=0,  bias=False),\n",
    "            nn.BatchNorm2d(32, affine=False),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1,  bias=False),\n",
    "            nn.BatchNorm2d(64, affine=False),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Conv2d(64, 32, kernel_size=1, stride=1, padding=0,  bias=False),\n",
    "            nn.BatchNorm2d(32, affine=False),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1,  bias=False),\n",
    "            nn.BatchNorm2d(64, affine=False),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            # CNNBlock 14x14\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2), \n",
    "            nn.Conv2d(64, 32, kernel_size=1, stride=1, padding=0,  bias=False),\n",
    "            nn.BatchNorm2d(32, affine=False),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1,  bias=False),\n",
    "            nn.BatchNorm2d(64, affine=False),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Conv2d(64, 32, kernel_size=1, stride=1, padding=0,  bias=False),\n",
    "            nn.BatchNorm2d(32, affine=False),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1,  bias=False),\n",
    "            nn.BatchNorm2d(64, affine=False),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1,  bias=False),\n",
    "            nn.BatchNorm2d(64, affine=False),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1,  bias=False),\n",
    "            nn.BatchNorm2d(64, affine=False),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            # CNNBlock 7x7\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1,  bias=False),\n",
    "            nn.BatchNorm2d(64, affine=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1,  bias=False),\n",
    "            nn.BatchNorm2d(64, affine=False),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            # CNNBlock Out\n",
    "            nn.Conv2d(64, 64, kernel_size=1, stride=1, padding=0,  bias=False),\n",
    "            nn.BatchNorm2d(64, affine=False),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Conv2d(64, 16, kernel_size=1, stride=1, padding=0,  bias=False),\n",
    "            nn.BatchNorm2d(16, affine=False),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Conv2d(16, 16, kernel_size=1, stride=1, padding=0,  bias=False),\n",
    "            nn.BatchNorm2d(16, affine=False),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Conv2d(16, self.B*5 + self.num_classes, kernel_size=1, stride=1, padding=0,  bias=False),\n",
    "            \n",
    "        )\n",
    "        return BED_model\n",
    "        \n",
    "          \n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_in',\n",
    "                    nonlinearity='relu'\n",
    "                )\n",
    "                if m.bias is not None:\n",
    "                        nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    # [xc1, yc1, w1, h1, conf1, xc2, yc2, w2, h2, conf2, smoke, fire]\n",
    "    # [0 ................. 4,    5 ................ 9      10    11 ]\n",
    "    def forward(self, x):\n",
    "        x_out = self.model(x)\n",
    "        x = x_out.permute(0, 2, 3, 1)\n",
    "        if self.B == 1:\n",
    "            class_softmax = torch.softmax(x[..., 5:7], dim=-1)\n",
    "            x = torch.cat((torch.sigmoid(x[..., 0:5]), class_softmax), dim=-1)  \n",
    "        else:\n",
    "            class_softmax = torch.softmax(x[..., 10:12], dim=-1)\n",
    "            x = torch.cat((torch.sigmoid(x[..., 0:10]), class_softmax), dim=-1)\n",
    "        return x "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9acb581-251c-4b6a-9560-8ec9cfb6b11d",
   "metadata": {},
   "source": [
    "# OPTIM BED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d4d48bb4-614d-4294-9da3-5cc76ad1aebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OPTIM_BED(nn.Module):\n",
    "    def __init__(self, num_classes, S, B, in_channels=3):\n",
    "        super(OPTIM_BED, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.num_classes = num_classes\n",
    "        self.S = S\n",
    "        self.B = B\n",
    "        \n",
    "        self.model = self.__create_BED__()\n",
    "\n",
    "        \n",
    "    def __create_BED__(self):\n",
    "        BED_model = nn.Sequential(\n",
    "            # Conv2d [in_channels, out_channels, kernel_size, stride, padding, bias]\n",
    "\n",
    "            # CNNBlock 224x224\n",
    "            nn.Conv2d(self.in_channels, 64, kernel_size=3, stride=1, padding=1,  bias=False),\n",
    "            nn.BatchNorm2d(64, affine=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout2d(p=0.3),\n",
    "\n",
    "            # CNNBlock 112x112\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(64, 24, kernel_size=3, stride=1, padding=1,  bias=False),\n",
    "            nn.BatchNorm2d(24, affine=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout2d(p=0.3),\n",
    "\n",
    "            # CNNBlock 56x56\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            # kernel = 1 in github\n",
    "            nn.Conv2d(24, 16, kernel_size=1, stride=1, padding=0,  bias=False),\n",
    "            nn.BatchNorm2d(16, affine=False),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1,  bias=False),\n",
    "            nn.BatchNorm2d(32, affine=False),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            # kernel = 1 in github\n",
    "            nn.Conv2d(32, 32, kernel_size=1, stride=1, padding=0,  bias=False),\n",
    "            nn.BatchNorm2d(32, affine=False),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1,  bias=False),\n",
    "            nn.BatchNorm2d(64, affine=False),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            # CNNBlock 28x28\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(64, 32, kernel_size=1, stride=1, padding=0,  bias=False),\n",
    "            nn.BatchNorm2d(32, affine=False),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1,  bias=False),\n",
    "            nn.BatchNorm2d(64, affine=False),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Conv2d(64, 32, kernel_size=1, stride=1, padding=0,  bias=False),\n",
    "            nn.BatchNorm2d(32, affine=False),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1,  bias=False),\n",
    "            nn.BatchNorm2d(64, affine=False),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Conv2d(64, 32, kernel_size=1, stride=1, padding=0,  bias=False),\n",
    "            nn.BatchNorm2d(32, affine=False),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1,  bias=False),\n",
    "            nn.BatchNorm2d(64, affine=False),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            # CNNBlock 14x14\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2), \n",
    "            nn.Conv2d(64, 32, kernel_size=1, stride=1, padding=0,  bias=False),\n",
    "            nn.BatchNorm2d(32, affine=False),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1,  bias=False),\n",
    "            nn.BatchNorm2d(64, affine=False),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Conv2d(64, 32, kernel_size=1, stride=1, padding=0,  bias=False),\n",
    "            nn.BatchNorm2d(32, affine=False),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1,  bias=False),\n",
    "            nn.BatchNorm2d(64, affine=False),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1,  bias=False),\n",
    "            nn.BatchNorm2d(64, affine=False),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1,  bias=False),\n",
    "            nn.BatchNorm2d(64, affine=False),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            # CNNBlock 7x7\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1,  bias=False),\n",
    "            nn.BatchNorm2d(128, affine=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1,  bias=False),\n",
    "            nn.BatchNorm2d(128, affine=False),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            # CNNBlock Out\n",
    "            nn.Conv2d(128, 128, kernel_size=1, stride=1, padding=0,  bias=False),\n",
    "            nn.BatchNorm2d(128, affine=False),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Conv2d(128, 64, kernel_size=1, stride=1, padding=0,  bias=False),\n",
    "            nn.BatchNorm2d(64, affine=False),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Conv2d(64, 16, kernel_size=1, stride=1, padding=0,  bias=False),\n",
    "            nn.BatchNorm2d(16, affine=False),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Conv2d(16, self.B*5 + self.num_classes, kernel_size=1, stride=1, padding=0,  bias=False),\n",
    "            \n",
    "        )\n",
    "        return BED_model\n",
    "        \n",
    "          \n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_in',\n",
    "                    nonlinearity='relu'\n",
    "                )\n",
    "                if m.bias is not None:\n",
    "                        nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    # [xc1, yc1, w1, h1, conf1, xc2, yc2, w2, h2, conf2, smoke, fire]\n",
    "    # [0 ................. 4,    5 ................ 9      10    11 ]\n",
    "    def forward(self, x):\n",
    "        x_out = self.model(x)\n",
    "        x = x_out.permute(0, 2, 3, 1)\n",
    "        if self.B == 1:\n",
    "            class_softmax = torch.softmax(x[..., 5:7], dim=-1)\n",
    "            x = torch.cat((torch.sigmoid(x[..., 0:5]), class_softmax), dim=-1)  \n",
    "        else:\n",
    "            class_softmax = torch.softmax(x[..., 10:12], dim=-1)\n",
    "            x = torch.cat((torch.sigmoid(x[..., 0:10]), class_softmax), dim=-1)\n",
    "        return x "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98be1fe-8fc5-45c8-82a2-7270a058699f",
   "metadata": {},
   "source": [
    "# BIG BED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "26f631ef-0823-4e9f-af72-e9a82d92b790",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BIG_BED(nn.Module):\n",
    "    def __init__(self, num_classes, S, B, in_channels=3):\n",
    "        super(BIG_BED, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.num_classes = num_classes\n",
    "        self.S = S\n",
    "        self.B = B\n",
    "        \n",
    "        self.model = self.__create_BED__()\n",
    "\n",
    "        \n",
    "    def __create_BED__(self):\n",
    "        BED_model = nn.Sequential(\n",
    "            # Conv2d [in_channels, out_channels, kernel_size, stride, padding, bias]\n",
    "\n",
    "            # CNNBlock 224x224\n",
    "            nn.Conv2d(self.in_channels, 128, kernel_size=3, stride=1, padding=1,  bias=False),\n",
    "            nn.BatchNorm2d(128, affine=False),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            # CNNBlock 112x112\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(128, 48, kernel_size=3, stride=1, padding=1,  bias=False),\n",
    "            nn.BatchNorm2d(48, affine=False),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            # CNNBlock 56x56\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            # kernel = 1 in github\n",
    "            nn.Conv2d(48, 32, kernel_size=1, stride=1, padding=0,  bias=False),\n",
    "            nn.BatchNorm2d(32, affine=False),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1,  bias=False),\n",
    "            nn.BatchNorm2d(64, affine=False),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            # kernel = 1 in github\n",
    "            nn.Conv2d(64, 64, kernel_size=1, stride=1, padding=0,  bias=False),\n",
    "            nn.BatchNorm2d(64, affine=False),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1,  bias=False),\n",
    "            nn.BatchNorm2d(128, affine=False),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            # CNNBlock 28x28\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(128, 64, kernel_size=1, stride=1, padding=0,  bias=False),\n",
    "            nn.BatchNorm2d(64, affine=False),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1,  bias=False),\n",
    "            nn.BatchNorm2d(128, affine=False),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Conv2d(128, 64, kernel_size=1, stride=1, padding=0,  bias=False),\n",
    "            nn.BatchNorm2d(64, affine=False),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1,  bias=False),\n",
    "            nn.BatchNorm2d(128, affine=False),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Conv2d(128, 64, kernel_size=1, stride=1, padding=0,  bias=False),\n",
    "            nn.BatchNorm2d(64, affine=False),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1,  bias=False),\n",
    "            nn.BatchNorm2d(128, affine=False),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            # CNNBlock 14x14\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2), \n",
    "            nn.Conv2d(128, 64, kernel_size=1, stride=1, padding=0,  bias=False),\n",
    "            nn.BatchNorm2d(64, affine=False),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1,  bias=False),\n",
    "            nn.BatchNorm2d(128, affine=False),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Conv2d(128, 64, kernel_size=1, stride=1, padding=0,  bias=False),\n",
    "            nn.BatchNorm2d(64, affine=False),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1,  bias=False),\n",
    "            nn.BatchNorm2d(128, affine=False),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1,  bias=False),\n",
    "            nn.BatchNorm2d(128, affine=False),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1,  bias=False),\n",
    "            nn.BatchNorm2d(128, affine=False),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            # CNNBlock 7x7\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1,  bias=False),\n",
    "            nn.BatchNorm2d(128, affine=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1,  bias=False),\n",
    "            nn.BatchNorm2d(128, affine=False),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            # CNNBlock Out\n",
    "            nn.Conv2d(128, 128, kernel_size=1, stride=1, padding=0,  bias=False),\n",
    "            nn.BatchNorm2d(128, affine=False),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Conv2d(128, 32, kernel_size=1, stride=1, padding=0,  bias=False),\n",
    "            nn.BatchNorm2d(32, affine=False),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Conv2d(32, 32, kernel_size=1, stride=1, padding=0,  bias=False),\n",
    "            nn.BatchNorm2d(32, affine=False),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Conv2d(32, self.B*5 + self.num_classes, kernel_size=1, stride=1, padding=0,  bias=True),\n",
    "            \n",
    "        )\n",
    "        return BED_model\n",
    "        \n",
    "          \n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_in',\n",
    "                    nonlinearity='relu'\n",
    "                )\n",
    "                if m.bias is not None:\n",
    "                        nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x_out = self.model(x)\n",
    "        x = x_out.permute(0, 2, 3, 1)\n",
    "        if self.B == 1:\n",
    "            class_softmax = torch.softmax(x[..., 5:7], dim=-1)\n",
    "            out = torch.cat((torch.sigmoid(x[..., 0:5]), class_softmax), dim=-1)\n",
    "        else:\n",
    "            class_softmax = torch.softmax(x[..., 10:12], dim=-1)\n",
    "            out = torch.cat((torch.sigmoid(x[..., 0:10]), class_softmax), dim=-1)   \n",
    "        return out "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02fea177-6d98-47b4-9cce-8981461388ab",
   "metadata": {},
   "source": [
    "# Yolo Loss 2BB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "42606acc-7693-488f-8f8a-4609738c1739",
   "metadata": {},
   "outputs": [],
   "source": [
    "class YoloLossMSE_2BBox(nn.Module):\n",
    "    '''\n",
    "    Calculates Yolo V1 loss function, detailed in the paper\n",
    "\n",
    "    Prediction format [xcell, ycell, w, h, confidence, class_0, class_1]\n",
    "    Future update to 2 BB: [xcell, ycell, w, h, confidence_A, xcell, ycell, w, h, confidence_B, class_0, class_1]\n",
    "    '''\n",
    "\n",
    "    def __init__(self, SX, SY, B, C):\n",
    "        super(YoloLossMSE_2BBox, self).__init__()\n",
    "        \n",
    "        self.mse = nn.MSELoss(reduction=\"sum\")\n",
    "\n",
    "        self.SX = SX\n",
    "        self.SY = SY\n",
    "        self.B = B\n",
    "        self.C = C\n",
    "\n",
    "        self.lambda_coord = 5\n",
    "        self.lambda_noobj = 0.5\n",
    "        self.lambda_conf = 1.5\n",
    "              \n",
    "        self.last_box_xy = 0.0\n",
    "        self.last_box_wh = 0.0\n",
    "        self.last_obj = 0.0\n",
    "        self.last_noobj =0.0\n",
    "        self.last_class = 0.0\n",
    "\n",
    "    def forward(self, ground_truth, predictions):\n",
    "\n",
    "        #predictions = predictions.reshape(-1, self.SY, self.SX, self.B*5 + self.C)\n",
    "        assert predictions.shape == (BATCH_SIZE, self.SY, self.SX, self.B*5 + self.C) \n",
    "        assert ground_truth.shape == (BATCH_SIZE, self.SY, self.SX, 5 + self.C) \n",
    "        \n",
    "        # =========================== #\n",
    "        #        Exists Box?          #\n",
    "        # =========================== #\n",
    "        exists_box = ground_truth[..., 4:5] == 1\n",
    "        pred_box1 = exists_box*predictions[..., 0:4]\n",
    "        pred_box2 = exists_box*predictions[..., 5:9]\n",
    "        target_box = exists_box*ground_truth[..., :4]\n",
    "\n",
    "        # =========================== #\n",
    "        #            IOU              #\n",
    "        # =========================== #\n",
    "        # Calculate IoU for the two predicted bounding boxes with target bbox\n",
    "        iou1 = iou_tensor(boxes_preds=pred_box1, \n",
    "                   boxes_labels=target_box,\n",
    "                   box_format=\"midpoint\")\n",
    "        #print(f'IOU 1\\n{iou1}')\n",
    "        iou2 = iou_tensor(boxes_preds=pred_box2, \n",
    "                   boxes_labels=target_box,\n",
    "                   box_format=\"midpoint\")\n",
    "        #print(f'IOU 2\\n{iou2}')\n",
    "        ious = torch.cat([iou1, iou2], dim=-1)\n",
    "        #print(f'IOUs\\n{ious}')\n",
    "        \n",
    "        iou_maxes, best_boxes = torch.max(ious, keepdim=True, dim=-1)\n",
    "        pred_boxes = best_boxes*pred_box2[..., :4]+(1-best_boxes)*pred_box1[..., :4]\n",
    "\n",
    "        # =============== #\n",
    "        #   Center Loss   #\n",
    "        # =============== #\n",
    "        xy_loss = self.mse(pred_boxes[..., :2],\n",
    "                           target_box[..., :2])\n",
    "        self.last_box_xy = xy_loss.item()\n",
    "        #print('\\nCenter Loss', center_loss)\n",
    "\n",
    "        # ====================== #\n",
    "        #   Width, Height Loss   #\n",
    "        # ====================== #\n",
    "        # Use torch.sign to undo torch.abs and preserve gradient sign \n",
    "        wh_loss = self.mse(torch.sign(pred_boxes[..., 2:4])*torch.sqrt(torch.abs(pred_boxes[..., 2:4])+1e-6),\n",
    "                           torch.sqrt(target_box[..., 2:4]))\n",
    "        #print(f'WH Loss {wh_loss:.6f}')\n",
    "        self.last_box_wh = wh_loss.item()\n",
    "\n",
    "        # =================== #\n",
    "        #   Confidence Loss   #\n",
    "        # =================== #\n",
    "        conf_pred_box = exists_box*( best_boxes*predictions[..., 9:10] + (1-best_boxes) * predictions[..., 4:5])\n",
    "        #print(f'Conf pred boxes \\n {conf_pred_box}')\n",
    "        # conf_loss = self.mse(conf_pred_box,\n",
    "        #                      iou_maxes)\n",
    "        conf_loss = self.mse(conf_pred_box,\n",
    "                             exists_box*ground_truth[..., 4:5])\n",
    "        #print(f'Conf loss {conf_loss:.6f}')\n",
    "        self.last_obj = conf_loss.item()\n",
    "\n",
    "        # ================== #\n",
    "        #   No Object Loss   #\n",
    "        # ================== #\n",
    "        noobj_box1 = self.mse((~exists_box)*predictions[..., 4:5],\n",
    "                              (~exists_box)*ground_truth[..., 4:5])\n",
    "        noobj_box2 = self.mse((~exists_box)*predictions[..., 9:10],\n",
    "                              (~exists_box)*ground_truth[..., 4:5])    \n",
    "        noobj_loss = noobj_box1 + noobj_box2\n",
    "        #print(f'No Obj loss {noobj_loss:.6f}')\n",
    "        self.last_noobj = noobj_loss.item()\n",
    "\n",
    "        # ======================= #\n",
    "        #   Classification Loss   #\n",
    "        # ======================= #\n",
    "        class_loss = self.mse(exists_box*predictions[..., 10:12],\n",
    "                              exists_box*ground_truth[..., 5:7]) \n",
    "        #print(f'Class Loss {class_loss:.6f}')\n",
    "        self.last_class = class_loss.item()\n",
    "\n",
    "        # ============== #\n",
    "        #   Total Loss   #\n",
    "        # ============== #\n",
    "        total_loss = (\n",
    "            self.lambda_coord*(xy_loss + wh_loss)\n",
    "            + self.lambda_conf*conf_loss\n",
    "            + self.lambda_noobj*noobj_loss \n",
    "            + class_loss\n",
    "        )\n",
    "\n",
    "        return total_loss\n",
    "\n",
    "    def get_last_losses(self):     \n",
    "        return (\n",
    "            self.lambda_coord*self.last_box_xy,\n",
    "            self.lambda_coord*self.last_box_wh,\n",
    "            self.lambda_conf*self.last_obj,\n",
    "            self.lambda_noobj*self.last_noobj,\n",
    "            self.last_class\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f040dbc-64fe-4bc6-bdad-2279ccf877d8",
   "metadata": {},
   "source": [
    "# YOLO Loss with CIOU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3f445c62-edde-4302-b8bc-55c2b181cbc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class YoloLossCIOU_2BBox(nn.Module):\n",
    "    '''\n",
    "    Calculates Yolo V1 loss function, detailed in the paper\n",
    "\n",
    "    Prediction format [xcell, ycell, w, h, confidence, class_0, class_1]\n",
    "    Future update to 2 BB: [xcell, ycell, w, h, confidence_A, xcell, ycell, w, h, confidence_B, class_0, class_1]\n",
    "    '''\n",
    "\n",
    "    def __init__(self, SX, SY, B, C):\n",
    "        super(YoloLossCIOU_2BBox, self).__init__()\n",
    "        \n",
    "        self.mse = nn.MSELoss(reduction=\"sum\")\n",
    "\n",
    "        self.SX = SX\n",
    "        self.SY = SY\n",
    "        self.B = B\n",
    "        self.C = C\n",
    "\n",
    "        self.lambda_coord = 5\n",
    "        self.lambda_noobj = 0.5\n",
    "        self.lambda_conf = 1.5\n",
    "        \n",
    "        self.box_ciou = 0.0\n",
    "        \n",
    "        self.last_obj = 0.0\n",
    "        self.last_noobj =0.0\n",
    "        self.last_class = 0.0\n",
    "        \n",
    "        self.cell2box_mask = self.get_cell2box_mask()\n",
    "        \n",
    "    def get_cell2box_mask(self):\n",
    "        dev = \"cuda\" if torch.cuda.is_available() else \"cpu\" \n",
    "        cell2box_mask = torch.zeros((SY, SX, 2), device=torch.device(dev)) \n",
    "        for i in range(SY):\n",
    "            for j in range(SX):\n",
    "                cell2box_mask[i,j,0] = j\n",
    "                cell2box_mask[i,j,1] = i\n",
    "        return cell2box_mask   \n",
    "\n",
    "    def forward(self, ground_truth, predictions):\n",
    "\n",
    "        #predictions = predictions.reshape(-1, self.SY, self.SX, self.B*5 + self.C)\n",
    "        assert predictions.shape == (BATCH_SIZE, self.SY, self.SX, self.B*5 + self.C) \n",
    "        assert ground_truth.shape == (BATCH_SIZE, self.SY, self.SX, 5 + self.C) \n",
    "        \n",
    "        # =========================== #\n",
    "        #        Exists Box?          #\n",
    "        # =========================== #\n",
    "        exists_box = ground_truth[..., 4:5] == 1\n",
    "        pred_box1 = exists_box*predictions[..., 0:4]\n",
    "        pred_box2 = exists_box*predictions[..., 5:9]\n",
    "        target_box = exists_box*ground_truth[..., :4]\n",
    "\n",
    "        # =========================== #\n",
    "        #            IOU              #\n",
    "        # =========================== #\n",
    "        # Calculate IoU for the two predicted bounding boxes with target bbox\n",
    "        iou1 = iou_tensor(boxes_preds=pred_box1, \n",
    "                   boxes_labels=target_box,\n",
    "                   box_format=\"midpoint\")\n",
    "        #print(f'IOU 1\\n{iou1}')\n",
    "        iou2 = iou_tensor(boxes_preds=pred_box2, \n",
    "                   boxes_labels=target_box,\n",
    "                   box_format=\"midpoint\")\n",
    "        #print(f'IOU 2\\n{iou2}')\n",
    "        ious = torch.cat([iou1, iou2], dim=-1)\n",
    "        #print(f'IOUs\\n{ious}')\n",
    "        \n",
    "        iou_maxes, best_boxes = torch.max(ious, keepdim=True, dim=-1)\n",
    "        pred_boxes = best_boxes*pred_box2[..., :4]+(1-best_boxes)*pred_box1[..., :4]\n",
    "        \n",
    "        # =============== #\n",
    "        #      CIOU       #\n",
    "        # =============== # \n",
    "        # Convert cells to boxes and concat\n",
    "        target_cell2box_x = (self.cell2box_mask[..., 0:1] + target_box[..., 0:1]) / self.SX \n",
    "        target_cell2box_y = (self.cell2box_mask[..., 1:2] + target_box[..., 1:2]) / self.SY\n",
    "        pred_cell2box_x = (self.cell2box_mask[..., 0:1] + pred_boxes[..., 0:1]) / self.SX\n",
    "        pred_cell2box_y = (self.cell2box_mask[..., 1:2] + pred_boxes[..., 1:2]) / self.SY\n",
    "        \n",
    "        target_cell2box = torch.concat((target_cell2box_x, target_cell2box_y, target_box[..., 2:4]), dim=-1)\n",
    "        pred_cell2box = torch.concat((pred_cell2box_x, pred_cell2box_y, pred_boxes[..., 2:4]), dim=-1)\n",
    "        \n",
    "        # Convert yolo [x, y, w, h] to [x1, y1, x2, y2]\n",
    "        target_x1 = ( target_cell2box[..., 0:1] - target_cell2box[..., 2:3] / 2 ).clamp(min=0)\n",
    "        target_y1 = ( target_cell2box[..., 1:2] - target_cell2box[..., 3:4] / 2 ).clamp(min=0)\n",
    "        target_x2 = ( target_cell2box[..., 0:1] + target_cell2box[..., 2:3] / 2 ).clamp(max=1)\n",
    "        target_y2 = ( target_cell2box[..., 1:2] + target_cell2box[..., 3:4] / 2 ).clamp(max=1)\n",
    "        target_xyxy = torch.concat((target_x1, target_y1, target_x2, target_y2, ground_truth[..., 4:5]), dim=-1)\n",
    "        # Zeroing all cells with no box, as we summed for cell2box to all cells\n",
    "        target_xyxy = exists_box * target_xyxy\n",
    "        \n",
    "        pred_x1 = ( pred_cell2box[..., 0:1] - pred_cell2box[..., 2:3] / 2 ).clamp(min=0)\n",
    "        pred_y1 = ( pred_cell2box[..., 1:2] - pred_cell2box[..., 3:4] / 2 ).clamp(min=0)\n",
    "        pred_x2 = ( pred_cell2box[..., 0:1] + pred_cell2box[..., 2:3] / 2 ).clamp(max=1)\n",
    "        pred_y2 = ( pred_cell2box[..., 1:2] + pred_cell2box[..., 3:4] / 2 ).clamp(max=1)\n",
    "        pred_xyxy = torch.concat((pred_x1, pred_y1, pred_x2, pred_y2), dim=-1)\n",
    "        pred_xyxy = exists_box * pred_xyxy\n",
    "        \n",
    "        # Reshape and calculate Loss CIOU\n",
    "        # For target, we keep confidence, to filter before CIOU\n",
    "        target_boxes_reshape = target_xyxy.reshape(-1, 5)\n",
    "        idx_nonzero = target_boxes_reshape[..., 4] == 1\n",
    "        target_boxes_filter = target_boxes_reshape[idx_nonzero, 0:4]\n",
    "        \n",
    "        pred_boxes_reshape = pred_xyxy.reshape(-1, 4)\n",
    "        pred_boxes_filter = pred_boxes_reshape[idx_nonzero, 0:4]\n",
    "        box_ciou_loss = complete_box_iou_loss(target_boxes_filter, pred_boxes_filter, reduction=\"sum\")\n",
    "        #box_ciou_loss = distance_box_iou_loss(target_boxes_filter, pred_boxes_filter, reduction=\"sum\")\n",
    "        if (np.isnan(box_ciou_loss.item())):\n",
    "            # print(f'Target boxes -> NaN\\n{target_boxes_filter}')\n",
    "            # print(f'Pred boxes -> NaN\\n{pred_boxes_filter}')\n",
    "            raise SystemExit(\"Stop right there!\")\n",
    "        self.box_ciou = box_ciou_loss.item()\n",
    "\n",
    "        # =================== #\n",
    "        #   Confidence Loss   #\n",
    "        # =================== #\n",
    "        conf_pred_box = exists_box*( best_boxes*predictions[..., 9:10] + (1-best_boxes) * predictions[..., 4:5])\n",
    "        #print(f'Conf pred boxes \\n {conf_pred_box}')\n",
    "        # conf_loss = self.mse(conf_pred_box,\n",
    "        #                      iou_maxes)\n",
    "        conf_loss = self.mse(conf_pred_box,\n",
    "                             exists_box*ground_truth[..., 4:5])\n",
    "        #print(f'Conf loss {conf_loss:.6f}')\n",
    "        self.last_obj = conf_loss.item()\n",
    "\n",
    "        # ================== #\n",
    "        #   No Object Loss   #\n",
    "        # ================== #\n",
    "        noobj_box1 = self.mse((~exists_box)*predictions[..., 4:5],\n",
    "                              (~exists_box)*ground_truth[..., 4:5])\n",
    "        noobj_box2 = self.mse((~exists_box)*predictions[..., 9:10],\n",
    "                              (~exists_box)*ground_truth[..., 4:5])    \n",
    "        noobj_loss = noobj_box1 + noobj_box2\n",
    "        #print(f'No Obj loss {noobj_loss:.6f}')\n",
    "        self.last_noobj = noobj_loss.item()\n",
    "\n",
    "        # ======================= #\n",
    "        #   Classification Loss   #\n",
    "        # ======================= #\n",
    "        class_loss = self.mse(exists_box*predictions[..., 10:12],\n",
    "                              exists_box*ground_truth[..., 5:7]) \n",
    "        #print(f'Class Loss {class_loss:.6f}')\n",
    "        self.last_class = class_loss.item()\n",
    "\n",
    "        # ============== #\n",
    "        #   Total Loss   #\n",
    "        # ============== #\n",
    "        total_loss = (\n",
    "            self.lambda_coord*box_ciou_loss\n",
    "            + self.lambda_conf*conf_loss\n",
    "            + self.lambda_noobj*noobj_loss \n",
    "            + class_loss\n",
    "        )\n",
    "\n",
    "        return total_loss\n",
    "\n",
    "    def get_last_losses(self):     \n",
    "        return (\n",
    "            self.lambda_coord*self.box_ciou,\n",
    "            self.lambda_conf*self.last_obj,\n",
    "            self.lambda_noobj*self.last_noobj,\n",
    "            self.last_class\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0707cc5-1db0-4a95-a183-cbad80adae89",
   "metadata": {},
   "source": [
    "# Train Loop Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4bada441-b601-4aa1-9274-ce50c616d1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Train function\n",
    "'''\n",
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']\n",
    "\n",
    "def train_fn(loader, model, optimizer, loss_fn, device):\n",
    "    \n",
    "    print(f'Learning Rate = {get_lr(optimizer=optimizer)}\\n')\n",
    "    logger.info(f'Learning Rate = {get_lr(optimizer=optimizer)}')\n",
    "\n",
    "    model.train()\n",
    "    loop = tqdm(loader, desc='Training', leave=True)\n",
    "    train_mean_loss = []\n",
    "    mean_box_loss = []\n",
    "    mean_confidence_loss = []\n",
    "    mean_noobj_loss = []\n",
    "    mean_class_loss = []\n",
    "\n",
    "    for batch_idx, (x, y) in enumerate(loop):\n",
    "#     for batch_idx, (x, y) in enumerate(loader):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        out = model(x)\n",
    "        train_loss = loss_fn(ground_truth=y, \n",
    "                             predictions=out)\n",
    "#         loss = loss_fn.forward(ground_truth=y, \n",
    "#                        predictions=out)\n",
    "\n",
    "        l1_lambda = 0.001  # hyperparameter for L1 regularization\n",
    "        l1_norm = sum(p.abs().sum() for p in model.parameters())\n",
    "        train_loss = train_loss + l1_lambda * l1_norm\n",
    "        \n",
    "        # Gradient Descent\n",
    "        optimizer.zero_grad()\n",
    "        train_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        # update progress bar\n",
    "        #loop.set_postfix(loss=loss.item())\n",
    "        #print(f'Partial Train Loss: {loss.item()}')\n",
    "\n",
    "        # MSE Loss\n",
    "        # xy_loss, wh_loss, obj_loss, noobj_loss, class_loss = loss_fn.get_last_losses()\n",
    "        # # Appending each loss\n",
    "        # train_mean_loss.append(train_loss.item())\n",
    "        # box_loss = xy_loss + wh_loss\n",
    "        # mean_box_loss.append(box_loss)\n",
    "        # mean_confidence_loss.append(obj_loss)\n",
    "        # mean_noobj_loss.append(noobj_loss)\n",
    "        # mean_class_loss.append(class_loss)\n",
    "        \n",
    "        # CIOU Loss\n",
    "        ciou_loss, obj_loss, noobj_loss, class_loss = loss_fn.get_last_losses()\n",
    "        #print(f'CIOU Loss of batch_idx {batch_idx} = {ciou_loss}')\n",
    "        # Appending each loss\n",
    "        train_mean_loss.append(train_loss.item())\n",
    "        mean_box_loss.append(ciou_loss)\n",
    "        mean_confidence_loss.append(obj_loss)\n",
    "        mean_noobj_loss.append(noobj_loss)\n",
    "        mean_class_loss.append(class_loss)\n",
    "\n",
    "    train_mean_loss_out = sum(train_mean_loss)/len(train_mean_loss)\n",
    "    #print(\"\\nTRAIN losses\")\n",
    "    logger.info(\"\\nTRAIN losses\")\n",
    "    mean_box_loss_out = sum(mean_box_loss)/len(mean_box_loss)\n",
    "    mean_confidence_loss_out = sum(mean_confidence_loss)/len(mean_confidence_loss)\n",
    "    mean_noobj_loss_out = sum(mean_noobj_loss)/len(mean_noobj_loss)\n",
    "    mean_class_loss_out = sum(mean_class_loss)/len(mean_class_loss)  \n",
    "\n",
    "    print(\"Total Loss\".ljust(12) + \"|\" + \n",
    "          \"Box Loss\".ljust(12) + \"|\" + \n",
    "          \"Conf Loss\".ljust(12) + \"|\" + \n",
    "          \"No Obj Loss\".ljust(12) + \"|\" + \n",
    "          \"Class Loss\".ljust(12))\n",
    "    print(\"------------\".ljust(12) + \" \" + \n",
    "          \"------------\".ljust(12) + \" \" + \n",
    "          \"------------\".ljust(12) + \" \" + \n",
    "          \"------------\".ljust(12) + \" \" + \n",
    "          \"------------\".ljust(12))\n",
    "    print(f'{train_mean_loss_out:.3f}'.ljust(12) + \"|\" +\n",
    "          f'{mean_box_loss_out:.3f}'.ljust(12) + \"|\" +\n",
    "          f'{mean_confidence_loss_out:.3f}'.ljust(12) + \"|\" +\n",
    "          f'{mean_noobj_loss_out:.3f}'.ljust(12) + \"|\" +\n",
    "          f'{mean_class_loss_out:.3f}'.ljust(12) + \"\\n\")\n",
    "    \n",
    "    logger.info(\"Total Loss\".ljust(12) + \"|\" + \n",
    "                  \"Box Loss\".ljust(12) + \"|\" + \n",
    "                  \"Conf Loss\".ljust(12) + \"|\" + \n",
    "                  \"No Obj Loss\".ljust(12) + \"|\" + \n",
    "                  \"Class Loss\".ljust(12))\n",
    "    logger.info(\"------------\".ljust(12) + \" \" + \n",
    "                  \"------------\".ljust(12) + \" \" + \n",
    "                  \"------------\".ljust(12) + \" \" + \n",
    "                  \"------------\".ljust(12) + \" \" + \n",
    "                  \"------------\".ljust(12))\n",
    "    logger.info(f'{train_mean_loss_out:.3f}'.ljust(12) + \"|\" +\n",
    "                  f'{mean_box_loss_out:.3f}'.ljust(12) + \"|\" +\n",
    "                  f'{mean_confidence_loss_out:.3f}'.ljust(12) + \"|\" +\n",
    "                  f'{mean_noobj_loss_out:.3f}'.ljust(12) + \"|\" +\n",
    "                  f'{mean_class_loss_out:.3f}'.ljust(12))\n",
    "    \n",
    "    return (train_mean_loss_out, \n",
    "            mean_box_loss_out, \n",
    "            mean_confidence_loss_out, \n",
    "            mean_noobj_loss_out, \n",
    "            mean_class_loss_out)\n",
    "    \n",
    "#     return train_mean_loss_out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0f4a05-7b83-45a0-8c9a-256d44095322",
   "metadata": {},
   "source": [
    "# Validation Loop Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "abce844b-bf71-40d1-a8e7-6a67c5e01a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Evaluation Function\n",
    "'''\n",
    "def eval_fn(loader, model, loss_fn, device):\n",
    "    \n",
    "    model.eval()\n",
    "    loop = tqdm(loader, desc='Validating', leave=True)\n",
    "    val_mean_loss = []\n",
    "    mean_box_loss = []\n",
    "    mean_confidence_loss = []\n",
    "    mean_noobj_loss = []\n",
    "    mean_class_loss = []\n",
    "\n",
    "    for batch_idx, (x, y) in enumerate(loop):\n",
    "#     for batch_idx, (x, y) in enumerate(loader):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        out = model(x)\n",
    "        val_loss = loss_fn(ground_truth=y, \n",
    "                           predictions=out)\n",
    "#         loss = loss_fn.forward(ground_truth=y, \n",
    "#                                predictions=out)\n",
    "        \n",
    "        # MSE Loss\n",
    "        # xy_loss, wh_loss, obj_loss, noobj_loss, class_loss = loss_fn.get_last_losses()\n",
    "        # # Appending each loss\n",
    "        # val_mean_loss.append(val_loss.item())\n",
    "        # box_loss = xy_loss + wh_loss\n",
    "        # mean_box_loss.append(box_loss)\n",
    "        # mean_confidence_loss.append(obj_loss)\n",
    "        # mean_noobj_loss.append(noobj_loss)\n",
    "        # mean_class_loss.append(class_loss)\n",
    "        \n",
    "        # CIOU Loss\n",
    "        ciou_loss, obj_loss, noobj_loss, class_loss = loss_fn.get_last_losses()\n",
    "        # Appending each loss\n",
    "        val_mean_loss.append(val_loss.item())\n",
    "        mean_box_loss.append(ciou_loss)\n",
    "        mean_confidence_loss.append(obj_loss)\n",
    "        mean_noobj_loss.append(noobj_loss)\n",
    "        mean_class_loss.append(class_loss)\n",
    "        \n",
    "        # update progress bar\n",
    "        #loop.set_postfix(loss=loss.item())\n",
    "\n",
    "    val_mean_loss_out = sum(val_mean_loss)/len(val_mean_loss)\n",
    "    #print(\"\\nVAL losses\")\n",
    "    logger.info(\"\\nVAL losses\")\n",
    "#     print(f\"Mean total loss was {val_mean_loss_out:.3f}\") # TQDM prints last batch loss\n",
    "    mean_box_loss_out = sum(mean_box_loss)/len(mean_box_loss)\n",
    "    mean_confidence_loss_out = sum(mean_confidence_loss)/len(mean_confidence_loss)\n",
    "    mean_noobj_loss_out = sum(mean_noobj_loss)/len(mean_noobj_loss)\n",
    "    mean_class_loss_out = sum(mean_class_loss)/len(mean_class_loss)  \n",
    "#     print(f\"Mean box loss was {mean_box_loss_out:.3f}\") \n",
    "#     print(f\"Mean confidence loss was {mean_confidence_loss_out:.3f}\") \n",
    "#     print(f\"Mean noobj loss was {mean_noobj_loss_out:.3f}\") \n",
    "#     print(f\"Mean class loss was {mean_class_loss_out:.3f}\") \n",
    "\n",
    "    print(\"Total Loss\".ljust(12) + \"|\" + \n",
    "          \"Box Loss\".ljust(12) + \"|\" + \n",
    "          \"Conf Loss\".ljust(12) + \"|\" + \n",
    "          \"No Obj Loss\".ljust(12) + \"|\" + \n",
    "          \"Class Loss\".ljust(12))\n",
    "    print(\"------------\".ljust(12) + \" \" + \n",
    "          \"------------\".ljust(12) + \" \" + \n",
    "          \"------------\".ljust(12) + \" \" + \n",
    "          \"------------\".ljust(12) + \" \" + \n",
    "          \"------------\".ljust(12))\n",
    "    print(f'{val_mean_loss_out:.3f}'.ljust(12) + \"|\" +\n",
    "          f'{mean_box_loss_out:.3f}'.ljust(12) + \"|\" +\n",
    "          f'{mean_confidence_loss_out:.3f}'.ljust(12) + \"|\" +\n",
    "          f'{mean_noobj_loss_out:.3f}'.ljust(12) + \"|\" +\n",
    "          f'{mean_class_loss_out:.3f}'.ljust(12))\n",
    "\n",
    "    logger.info(\"Total Loss\".ljust(12) + \"|\" + \n",
    "                  \"Box Loss\".ljust(12) + \"|\" + \n",
    "                  \"Conf Loss\".ljust(12) + \"|\" + \n",
    "                  \"No Obj Loss\".ljust(12) + \"|\" + \n",
    "                  \"Class Loss\".ljust(12))\n",
    "    logger.info(\"------------\".ljust(12) + \" \" + \n",
    "                  \"------------\".ljust(12) + \" \" + \n",
    "                  \"------------\".ljust(12) + \" \" + \n",
    "                  \"------------\".ljust(12) + \" \" + \n",
    "                  \"------------\".ljust(12))\n",
    "    logger.info(f'{val_mean_loss_out:.3f}'.ljust(12) + \"|\" +\n",
    "                  f'{mean_box_loss_out:.3f}'.ljust(12) + \"|\" +\n",
    "                  f'{mean_confidence_loss_out:.3f}'.ljust(12) + \"|\" +\n",
    "                  f'{mean_noobj_loss_out:.3f}'.ljust(12) + \"|\" +\n",
    "                  f'{mean_class_loss_out:.3f}'.ljust(12))\n",
    "    \n",
    "    return (val_mean_loss_out, \n",
    "            mean_box_loss_out, \n",
    "            mean_confidence_loss_out, \n",
    "            mean_noobj_loss_out, \n",
    "            mean_class_loss_out)\n",
    "    \n",
    "#     return val_loss_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d47f51-5c7b-434a-af27-a0e2596596bc",
   "metadata": {},
   "source": [
    "# Hyperparameters and Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "79142328-c910-4425-afd7-f6d38bd9baf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' ============================\n",
    "    Hyperparameters and More\n",
    "============================ '''\n",
    "\n",
    "LEARNING_RATE = 5e-4\n",
    "#LEARNING_RATE = 1e-1\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "BATCH_SIZE = 64 \n",
    "WEIGHT_DECAY = 1e-4\n",
    "#WEIGHT_DECAY = 0\n",
    "EPOCHS = 10\n",
    "NUM_WORKERS = 2\n",
    "PIN_MEMORY = True\n",
    "#LOAD_MODEL = False\n",
    "#LOAD_MODEL_FILE = \"overfit.pth.tar\"\n",
    "\n",
    "TRAIN_IMG_DIR = train_imgs\n",
    "TRAIN_LABEL_DIR = train_labels\n",
    "EVAL_IMG_DIR = val_imgs\n",
    "EVAL_LABEL_DIR = val_labels\n",
    "\n",
    "LOG_FILE = \"log_file.xlsx\"\n",
    "\n",
    "IOU_mAP_THRES = 0.5\n",
    "IOU_NMS_THRES = 0.3\n",
    "THRESHOLD = 0.2\n",
    "\n",
    "''' ============================\n",
    "    Configuration\n",
    "============================ '''\n",
    "IMG_W = IMG_DIM['W']\n",
    "IMG_H = IMG_DIM['H']\n",
    "\n",
    "CELL_SIZE_X = 1/SX\n",
    "CELL_SIZE_Y = 1/SY\n",
    "\n",
    "\n",
    "''' ============================\n",
    "    Metrics\n",
    "============================ '''\n",
    "# mAP output\n",
    "## Train\n",
    "train_mAP = []\n",
    "train_class_AP = []\n",
    "train_class_precision = []\n",
    "train_class_recall = []\n",
    "## Val\n",
    "val_mAP = []\n",
    "val_class_AP = []\n",
    "val_class_precision = []\n",
    "val_class_recall = []\n",
    "\n",
    "# Loss output\n",
    "## Train\n",
    "train_total_loss = []\n",
    "train_box_loss = []\n",
    "train_confidence_loss = []\n",
    "train_noobj_loss = []\n",
    "train_class_loss = []\n",
    "## Val\n",
    "val_total_loss = []\n",
    "val_box_loss = []\n",
    "val_confidence_loss = []\n",
    "val_noobj_loss = []\n",
    "val_class_loss = []\n",
    "\n",
    "epochs_plot = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba95ccfe-a797-45cd-82bb-37bcc9af62ff",
   "metadata": {},
   "source": [
    "# Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "22ef85f5-39b7-473a-855b-0561de058bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "\n",
    "    ''' ============================\n",
    "        Print Config Values\n",
    "    ============================ '''\n",
    "    print(f'Device: {DEVICE}')\n",
    "    print(f'Learning Rate: {LEARNING_RATE}')\n",
    "    print(f'Batch Size: {BATCH_SIZE}')\n",
    "    print(f'IMG DIMS: ({IMG_H}, {IMG_W})')\n",
    "    print(f'W: {IMG_W}\\nH: {IMG_H}')\n",
    "    print(f'SX: {SX}\\nSY: {SY}\\nB: {B}\\nC: {C}')\n",
    "    print(f'Cell size:\\n\\tx = {CELL_SIZE_X}\\n\\ty = {CELL_SIZE_Y}')\n",
    "    print(f'\\nConfidence Threshold: {THRESHOLD}')\n",
    "    print(f'IOU mAP Threshold: {IOU_mAP_THRES}')\n",
    "    print(f'IOU NMS Threshold: {IOU_NMS_THRES}\\n')\n",
    "\n",
    "    logger.info(f'Device: {DEVICE}')\n",
    "    logger.info(f'Learning Rate: {LEARNING_RATE}')\n",
    "    logger.info(f'Batch Size: {BATCH_SIZE}')\n",
    "    logger.info(f'IMG DIMS: ({IMG_H}, {IMG_W})')\n",
    "    logger.info(f'W: {IMG_W}\\nH: {IMG_H}')\n",
    "    logger.info(f'SX: {SX}\\nSY: {SY}\\nB: {B}\\nC: {C}')\n",
    "    logger.info(f'\\nConfidence Threshold: {THRESHOLD}')\n",
    "    logger.info(f'IOU mAP Threshold: {IOU_mAP_THRES}')\n",
    "    logger.info(f'IOU NMS Threshold: {IOU_NMS_THRES}\\n')\n",
    "    #logger.info(f'Cell size:\\n\\tx = {CELL_SIZE_X}\\n\\ty = {CELL_SIZE_Y}')\n",
    "\n",
    "    ''' ============================\n",
    "        Cell to Box Mask\n",
    "    ============================ '''\n",
    "    cell2box_mask = torch.zeros((SY, SX, 2))\n",
    "    for i in range(SY):\n",
    "        for j in range(SX):\n",
    "            cell2box_mask[i,j,0] = j\n",
    "            cell2box_mask[i,j,1] = i    \n",
    "#     print(cell2box_mask)\n",
    "#     print(\"\\n\")\n",
    "\n",
    "    ''' ======================================\n",
    "        SETUP: Model, Loss, Dataset, Loader\n",
    "    ====================================== '''\n",
    "    if IMG_W == 88:\n",
    "        print(\"Using Tinyssimo 88x88\")\n",
    "        logger.info(\"Using Tinyssimo 88x88\")\n",
    "        #model_tynss = Tinyssimo_fixed_88x88(num_classes=C).to(DEVICE)\n",
    "        model_tynss = Tinyssimo_fixed_88x88_BatchNorm(num_classes=C).to(DEVICE)\n",
    "        #model_tynss = Tinyssimo_fixed_88x88_NANO(num_classes=C).to(DEVICE)\n",
    "    elif IMG_W == 448:\n",
    "        print(\"Using Tinyssimo 448x448\")\n",
    "        logger.info(\"Using Tinyssimo 448x448\")\n",
    "        model_tynss = Tinyssimo_fixed_448x448(num_classes=C).to(DEVICE)\n",
    "    elif IMG_W == 224:\n",
    "        print(\"Using OPTIM BED 224x224\")\n",
    "        logger.info(\"Using OPTIM BED 224x224\")\n",
    "        model_tynss = OPTIM_BED(num_classes = C,\n",
    "                          S = SX,\n",
    "                          B = B,\n",
    "                          in_channels = 3).to(DEVICE)\n",
    "    else:\n",
    "        print(\"Using Tinyssimo 160x120\")\n",
    "        logger.info(\"Using Tinyssimo 160x120\")\n",
    "        model_tynss = Tinyssimo_fixed_160x120(num_classes=C).to(DEVICE)\n",
    "    \n",
    "    model_tynss._initialize_weights()\n",
    "\n",
    "    # Check model shape\n",
    "    in_rand_np = np.random.rand(4, 3, IMG_H, IMG_W)\n",
    "    in_rand = torch.tensor(in_rand_np, dtype=torch.float32, device=DEVICE)\n",
    "    out_test = model_tynss(in_rand)\n",
    "    print(f'Model shape is {out_test.shape}')\n",
    "    print(f'BED Model Arquitecture\\n{model_tynss}')\n",
    "    logger.info(f'Model shape is {out_test.shape}')\n",
    "    logger.info(f'BED Model Arquitecture\\n{model_tynss}')   \n",
    "    \n",
    "    optimizer = optim.Adam(model_tynss.parameters(), \n",
    "                           lr=LEARNING_RATE, \n",
    "                           weight_decay=WEIGHT_DECAY)\n",
    "    \n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, \n",
    "                                                     mode='min',\n",
    "                                                     factor=0.8, \n",
    "                                                     patience=2, \n",
    "                                                     threshold=0.1, \n",
    "                                                     threshold_mode='abs',\n",
    "                                                     min_lr=1e-6)\n",
    "    \n",
    "#     optimizer = torch.optim.SGD(model_tynss.parameters(), \n",
    "#                                 lr=LEARNING_RATE,\n",
    "#                                 momentum=0.9, \n",
    "#                                 weight_decay=WEIGHT_DECAY, \n",
    "#                                 nesterov=True)\n",
    "    \n",
    "    n_trainable = sum(p.numel() for p in model_tynss.parameters() if p.requires_grad)\n",
    "    print(f'\\nTrainable parameters = {n_trainable}')\n",
    "    logger.info(f'\\nTrainable parameters = {n_trainable}')\n",
    "\n",
    "    n_params = parameters_to_vector(model_tynss.parameters()).numel()\n",
    "    print(f'Total parameters = {n_params}\\n')\n",
    "    logger.info(f'Total parameters = {n_params}\\n')\n",
    "    \n",
    "    if B == 1:\n",
    "        print(f'Using YoloLossMSE')\n",
    "        logger.info(f'Using YoloLossMSE')\n",
    "        loss_fn = YoloLossMSE(SX=SX, \n",
    "                              SY=SY, \n",
    "                              B=B, \n",
    "                              C=C)\n",
    "    elif B == 2:\n",
    "        print(f'Using YoloLossCIOU_2BBox')\n",
    "        logger.info(f'Using YoloLossCIOU_2BBox')\n",
    "        # loss_fn = YoloLossMSE_2BBox(SX=SX, \n",
    "        #                             SY=SY, \n",
    "        #                             B=B, \n",
    "        #                             C=C)\n",
    "        \n",
    "        loss_fn =YoloLossCIOU_2BBox(SX=SX, \n",
    "                                    SY=SY, \n",
    "                                    B=B, \n",
    "                                    C=C)\n",
    "    else:\n",
    "        print(\"Wrong B bounding boxes configuration\")\n",
    "        logger.info(\"Wrong B bounding boxes configuration\")\n",
    "\n",
    "    train_transform = A.Compose([\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        # If boxes are to close, it can remove some because they fall inside same cell\n",
    "        A.RandomSizedBBoxSafeCrop(height=int(1.4*IMG_H),\n",
    "                                  width= int(1.4*IMG_W),\n",
    "                                  erosion_rate=0.6,\n",
    "                                  p=0.3),\n",
    "        A.OneOf([\n",
    "            A.RandomBrightnessContrast(p=0.4),\n",
    "            A.HueSaturationValue(hue_shift_limit=10, p=0.2),\n",
    "            A.Blur(blur_limit=(3,3), p=0.3),\n",
    "            A.CLAHE(clip_limit=2.0, p=0.3),\n",
    "        ], p=0.9),\n",
    "            # Shifting, scaling and rotation could dive 2 bbox inside same grid...\n",
    "            #A.ShiftScaleRotate(rotate_limit=10, p=0.2),\n",
    "        A.Resize(IMG_H, IMG_W, p=1),\n",
    "        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), p=1),\n",
    "        ToTensorV2(p=1),\n",
    "    ], bbox_params=A.BboxParams(format='yolo', \n",
    "                                min_area=8*8, \n",
    "                                min_visibility=0.05, \n",
    "                                label_fields=['class_labels']))\n",
    "    # TRAIN\n",
    "    print(\"\\nTRAIN DFIRE dataset\")\n",
    "    logger.info(\"\\nTRAIN DFIRE dataset\")\n",
    "    train_dataset = DFireDataset(img_h = IMG_H,\n",
    "                                 img_w = IMG_W,\n",
    "                                 img_dir = TRAIN_IMG_DIR,\n",
    "                                 label_dir = TRAIN_LABEL_DIR,\n",
    "                                 SX = SX,\n",
    "                                 SY = SY,\n",
    "                                 C = C,\n",
    "                                 transform=train_transform)\n",
    "    \n",
    "    print(\"TRAIN DFS dataset\")\n",
    "    logger.info(\"TRAIN DFS dataset\")\n",
    "    train_dfs_dataset = DFSDataset(img_h = IMG_H,\n",
    "                                   img_w = IMG_W,\n",
    "                                   img_dir = dfs_images_dir,\n",
    "                                   labels_list = dfs_train_list,\n",
    "                                   SX = SX,\n",
    "                                   SY = SY,\n",
    "                                   C = C,\n",
    "                                   transform=train_transform)\n",
    "\n",
    "    print(\"Concatenate Train DFire and DFS datasets\")\n",
    "    logger.info(\"Concatenate Train DFire and DFS datasets\")\n",
    "    full_train_ds = torch.utils.data.ConcatDataset((train_dataset, train_dfs_dataset))\n",
    "    print(f'Train dataset len: {len(full_train_ds)}')\n",
    "    logger.info(f'Train dataset len: {len(full_train_ds)}')\n",
    "\n",
    "    \n",
    "    # VALIDATION\n",
    "    val_transform = A.Compose([\n",
    "        A.Resize(IMG_H, IMG_W, p=1),\n",
    "        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), p=1),\n",
    "        ToTensorV2(p=1),\n",
    "    ], bbox_params=A.BboxParams(format='yolo', label_fields=['class_labels']))\n",
    "    \n",
    "    print(\"\\nTEST DFire dataset\")\n",
    "    logger.info(\"\\nTEST DFire dataset\")\n",
    "    eval_dataset = DFireDataset(img_h = IMG_H,\n",
    "                                img_w = IMG_W,\n",
    "                                img_dir = EVAL_IMG_DIR,\n",
    "                                label_dir = EVAL_LABEL_DIR,\n",
    "                                SX = SX,\n",
    "                                SY = SY,\n",
    "                                C = C,\n",
    "                                transform=val_transform)\n",
    "    print(\"TEST DFS dataset\")\n",
    "    logger.info(\"TEST DFS dataset\")\n",
    "    test_dfs_dataset = DFSDataset(img_h = IMG_H,\n",
    "                                  img_w = IMG_W,\n",
    "                                  img_dir = dfs_images_dir,\n",
    "                                  labels_list = dfs_test_list,\n",
    "                                  SX = SX,\n",
    "                                  SY = SY,\n",
    "                                  C = C,\n",
    "                                  transform=val_transform)\n",
    "    \n",
    "    print(\"Concatenate Test DFire and DFS datasets\")\n",
    "    logger.info(\"Concatenate Test DFire and DFS datasets\")\n",
    "    full_test_ds = torch.utils.data.ConcatDataset((eval_dataset, test_dfs_dataset))\n",
    "    print(f'Test dataset len: {len(full_test_ds)}')\n",
    "    logger.info(f'Test dataset len: {len(full_test_ds)}')\n",
    "    \n",
    "    \n",
    "    # LOADERS\n",
    "    train_loader = DataLoader(dataset=full_train_ds,\n",
    "                              batch_size=BATCH_SIZE,\n",
    "                              num_workers=NUM_WORKERS,\n",
    "                              pin_memory=PIN_MEMORY,\n",
    "                              shuffle=True,\n",
    "                              drop_last=True)\n",
    "\n",
    "    eval_loader = DataLoader(dataset=full_test_ds,\n",
    "                             batch_size=BATCH_SIZE,\n",
    "                             num_workers=NUM_WORKERS,\n",
    "                             pin_memory=PIN_MEMORY,\n",
    "                             shuffle=False,\n",
    "                             drop_last=True)\n",
    "\n",
    "\n",
    "    ''' ==============================================================\n",
    "                                TRAINING LOOP\n",
    "    ============================================================== '''\n",
    "    # Start with infinite validation loss\n",
    "    best_valid_loss = np.inf\n",
    "    best_mAP = -1\n",
    "\n",
    "    start = datetime.datetime.now()\n",
    "    start_time = start.strftime(\"%H:%M:%S\")\n",
    "    print(f'\\n***Start Training: {start_time}\\n')\n",
    "    logger.info(f'\\n***Start Training: {start_time}\\n')\n",
    "    \n",
    "    for epoch in range(EPOCHS):\n",
    "\n",
    "        print(f'\\n=== EPOCH {epoch}/{EPOCHS-1} ===')\n",
    "        logger.info(f'\\n=== EPOCH {epoch}/{EPOCHS-1} ===')\n",
    "        \n",
    "        # TRAINING\n",
    "        t_loss, b_loss, conf_loss, n_loss, cls_loss = train_fn(loader=train_loader, \n",
    "                                                               model=model_tynss, \n",
    "                                                               optimizer=optimizer, \n",
    "                                                               loss_fn=loss_fn,\n",
    "                                                               device=DEVICE)\n",
    "        # Appending Train Losses\n",
    "        train_total_loss.append(t_loss)\n",
    "        train_box_loss.append(b_loss)\n",
    "        train_confidence_loss.append(conf_loss)\n",
    "        train_noobj_loss.append(n_loss)\n",
    "        train_class_loss.append(cls_loss)\n",
    "        \n",
    "        ##### NO LOGS, ONLY TOTAL LOSS\n",
    "#         t_loss = train_fn(loader=train_loader, \n",
    "#                           model=model_tynss, \n",
    "#                           optimizer=optimizer, \n",
    "#                           loss_fn=loss_fn,\n",
    "#                           device=DEVICE)\n",
    "#         # Appending Train Losses\n",
    "#         train_total_loss.append(t_loss)\n",
    "\n",
    "\n",
    "        # TRAIN MEAN AVERAGE PRECISION\n",
    "        if ( (epoch+1) % 15 ) == 0:\n",
    "            pred_boxes, target_boxes = get_bboxes(loader=train_loader, \n",
    "                                                  model=model_tynss,\n",
    "                                                  SX=SX,\n",
    "                                                  SY=SY,\n",
    "                                                  B=B,\n",
    "                                                  C=C,\n",
    "                                                  mask=cell2box_mask,\n",
    "                                                  iou_threshold=IOU_NMS_THRES, \n",
    "                                                  threshold=THRESHOLD,\n",
    "                                                  device=DEVICE,\n",
    "                                                  box_format=\"midpoint\")\n",
    "\n",
    "            mean_avg_prec, avg_prec, cls_prec, cls_rec = mAP(pred_boxes=pred_boxes, \n",
    "                                                             true_boxes=target_boxes, \n",
    "                                                             iou_threshold=IOU_mAP_THRES, \n",
    "                                                             box_format=\"midpoint\",\n",
    "                                                             num_classes=C)\n",
    "\n",
    "            train_mAP.append(mean_avg_prec)\n",
    "            train_class_AP.append(avg_prec)   \n",
    "            train_class_precision.append(cls_prec)\n",
    "            train_class_recall.append(cls_rec)\n",
    "            print(f\"\\nTrain mAP: {mean_avg_prec:.3f}\")\n",
    "            logger.info(f\"\\nTrain mAP: {mean_avg_prec:.3f}\")\n",
    "        \n",
    "        # VALIDATING\n",
    "        with torch.no_grad():\n",
    "            v_loss, b_loss, conf_loss, n_loss, cls_loss = eval_fn(loader=eval_loader, \n",
    "                                                                  model=model_tynss,                         \n",
    "                                                                  loss_fn=loss_fn,\n",
    "                                                                  device=DEVICE)\n",
    "            # Appending Validation Losses\n",
    "            val_total_loss.append(v_loss)\n",
    "            val_box_loss.append(b_loss)\n",
    "            val_confidence_loss.append(conf_loss)\n",
    "            val_noobj_loss.append(n_loss)\n",
    "            val_class_loss.append(cls_loss)\n",
    "            \n",
    "            scheduler.step(v_loss)\n",
    "            \n",
    "            ##### NO LOGS, ONLY TOTAL LOSS\n",
    "#             v_loss = eval_fn(loader=eval_loader, \n",
    "#                              model=model_tynss,                         \n",
    "#                              loss_fn=loss_fn,\n",
    "#                              device=DEVICE)\n",
    "#             # Appending Validation Losses\n",
    "#             val_total_loss.append(v_loss)\n",
    "\n",
    "            # VALIDATION MEAN AVERAGE PRECISION\n",
    "            if ( (epoch+1) % 5 ) == 0:\n",
    "                pred_boxes, target_boxes = get_bboxes(loader=eval_loader, \n",
    "                                                      model=model_tynss,\n",
    "                                                      SX=SX,\n",
    "                                                      SY=SY,\n",
    "                                                      B=B,\n",
    "                                                      C=C,\n",
    "                                                      mask=cell2box_mask,\n",
    "                                                      iou_threshold=IOU_NMS_THRES, \n",
    "                                                      threshold=THRESHOLD,\n",
    "                                                      device=DEVICE,\n",
    "                                                      box_format=\"midpoint\")\n",
    "\n",
    "                mean_avg_prec, avg_prec, cls_prec, cls_rec = mAP(pred_boxes=pred_boxes, \n",
    "                                                                 true_boxes=target_boxes, \n",
    "                                                                 iou_threshold=IOU_mAP_THRES, \n",
    "                                                                 box_format=\"midpoint\",\n",
    "                                                                 num_classes=C)\n",
    "\n",
    "                val_mAP.append(mean_avg_prec)\n",
    "                val_class_AP.append(avg_prec)   \n",
    "                val_class_precision.append(cls_prec)\n",
    "                val_class_recall.append(cls_rec)\n",
    "                print(f\"\\nValidation mAP: {mean_avg_prec:.3f}\")\n",
    "                logger.info(f\"\\nValidation mAP: {mean_avg_prec:.3f}\")\n",
    "\n",
    "                epochs_plot.append(epoch)\n",
    "                plt.plot(epochs_plot, val_mAP, label=\"Val mAP\")\n",
    "                plt.title(\"Val mAP:@.50\")\n",
    "                plt.ylim([0,1])\n",
    "                plt.xlabel(\"Epochs\")\n",
    "                plt.ylabel(\"mAP:@.50\")\n",
    "                plt.legend()\n",
    "                plt.grid()\n",
    "                plt.savefig(log_path + \"Temp_bed_mAP.png\")\n",
    "                plt.close()\n",
    "\n",
    "                if best_mAP < mean_avg_prec:\n",
    "                    best_mAP = mean_avg_prec\n",
    "                    print(f\"\\nSaving model with new best val mAP: {best_mAP:.3f}\")\n",
    "                    logger.info(f\"\\nSaving model with new best val mAP: {best_mAP:.3f}\")\n",
    "                    torch.save({\n",
    "                        'epoch': epoch,\n",
    "                        'model_state_dict': model_tynss.state_dict(),\n",
    "                        'optimizer_state_dict': optimizer.state_dict(),}, log_path + 'bed_best_2BB_mAP.pt')  \n",
    "\n",
    "        if ( (epoch+1) % 5 ) == 0:\n",
    "            torch.save(model_tynss.state_dict(), log_path + 'bed_2BB_5epoch.pt')\n",
    "            \n",
    "        if best_valid_loss > v_loss:\n",
    "            best_valid_loss = v_loss\n",
    "            print(f\"\\nSaving model with new best validation loss: {best_valid_loss:.3f}\")\n",
    "            logger.info(f\"\\nSaving model with new best validation loss: {best_valid_loss:.3f}\")\n",
    "            torch.save(model_tynss.state_dict(), log_path + 'best_2BB.pt')     \n",
    "        \n",
    "    logger.info('Saving last model')   \n",
    "    torch.save(model_tynss.state_dict(), log_path + 'last_2BB.pt') \n",
    "    \n",
    "#     save_log(epochs=EPOCHS, \n",
    "#              train_total_loss=train_total_loss,\n",
    "#              train_box_loss=train_box_loss,\n",
    "#              train_class_loss=train_class_loss,\n",
    "#              train_confidence_loss=train_confidence_loss,\n",
    "#              train_noobj_loss=train_noobj_loss,\n",
    "#              train_mAP=train_mAP,\n",
    "#              train_class_AP=train_class_AP,\n",
    "#              train_class_precision=train_class_precision,\n",
    "#              train_class_recall=train_class_recall,\n",
    "#              val_total_loss=val_total_loss,\n",
    "#              val_box_loss=val_box_loss,\n",
    "#              val_class_loss=val_class_loss,\n",
    "#              val_confidence_loss=val_confidence_loss,\n",
    "#              val_noobj_loss=val_noobj_loss,\n",
    "#              val_mAP=val_mAP,\n",
    "#              val_class_AP=val_class_AP,\n",
    "#              val_class_precision=val_class_precision,\n",
    "#              val_class_recall=val_class_recall,\n",
    "#              log_file_dst=LOG_FILE)\n",
    "\n",
    "    end = datetime.datetime.now()\n",
    "    end_time = end.strftime(\"%H:%M:%S\")\n",
    "    print(f'\\n***Script finished: {end_time}\\n')  \n",
    "    print(f'Time elapsed: {end-start}')\n",
    "    logger.info(f'\\n***Script finished: {end_time}\\n')  \n",
    "    logger.info(f'Time elapsed: {end-start}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae580f4-e175-4035-a2f7-dabd179db9a3",
   "metadata": {},
   "source": [
    "# Main Execute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "28659625-6ba4-44ac-ad2c-576b16425dcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting script\n",
      "\n",
      "Device: cuda\n",
      "Learning Rate: 0.0005\n",
      "Batch Size: 64\n",
      "IMG DIMS: (224, 224)\n",
      "W: 224\n",
      "H: 224\n",
      "SX: 7\n",
      "SY: 7\n",
      "B: 2\n",
      "C: 2\n",
      "Cell size:\n",
      "\tx = 0.14285714285714285\n",
      "\ty = 0.14285714285714285\n",
      "\n",
      "Confidence Threshold: 0.2\n",
      "IOU mAP Threshold: 0.5\n",
      "IOU NMS Threshold: 0.3\n",
      "\n",
      "Using OPTIM BED 224x224\n",
      "Model shape is torch.Size([4, 7, 7, 12])\n",
      "BED Model Arquitecture\n",
      "OPTIM_BED(\n",
      "  (model): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout2d(p=0.3, inplace=False)\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (5): Conv2d(64, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (6): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "    (7): ReLU()\n",
      "    (8): Dropout2d(p=0.3, inplace=False)\n",
      "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (10): Conv2d(24, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (11): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "    (12): ReLU()\n",
      "    (13): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (14): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "    (15): ReLU()\n",
      "    (16): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (17): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "    (18): ReLU()\n",
      "    (19): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (20): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "    (21): ReLU()\n",
      "    (22): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (23): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (24): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "    (25): ReLU()\n",
      "    (26): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (27): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "    (28): ReLU()\n",
      "    (29): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (30): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "    (31): ReLU()\n",
      "    (32): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (33): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "    (34): ReLU()\n",
      "    (35): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (36): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "    (37): ReLU()\n",
      "    (38): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (39): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "    (40): ReLU()\n",
      "    (41): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (42): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (43): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "    (44): ReLU()\n",
      "    (45): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (46): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "    (47): ReLU()\n",
      "    (48): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (49): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "    (50): ReLU()\n",
      "    (51): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (52): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "    (53): ReLU()\n",
      "    (54): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (55): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "    (56): ReLU()\n",
      "    (57): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (58): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "    (59): ReLU()\n",
      "    (60): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (61): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (62): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "    (63): ReLU()\n",
      "    (64): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (65): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "    (66): ReLU()\n",
      "    (67): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (68): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "    (69): ReLU()\n",
      "    (70): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (71): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "    (72): ReLU()\n",
      "    (73): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (74): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "    (75): ReLU()\n",
      "    (76): Conv2d(16, 12, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  )\n",
      ")\n",
      "\n",
      "Trainable parameters = 463104\n",
      "Total parameters = 463104\n",
      "\n",
      "Using YoloLossCIOU_2BBox\n",
      "\n",
      "TRAIN DFIRE dataset\n",
      "Removed wrong images: 0\n",
      "Removed due to overlapping: 546\n",
      "Removed due to more than 3: 1764\n",
      "TRAIN DFS dataset\n",
      "Removed wrong images: 0\n",
      "Removed due to overlapping: 273\n",
      "Removed due to more than 3: 1188\n",
      "Concatenate Train DFire and DFS datasets\n",
      "Train dataset len: 21158\n",
      "\n",
      "TEST DFire dataset\n",
      "Removed wrong images: 0\n",
      "Removed due to overlapping: 118\n",
      "Removed due to more than 3: 445\n",
      "TEST DFS dataset\n",
      "Removed wrong images: 0\n",
      "Removed due to overlapping: 73\n",
      "Removed due to more than 3: 320\n",
      "Concatenate Test DFire and DFS datasets\n",
      "Test dataset len: 5284\n",
      "\n",
      "***Start Training: 09:11:16\n",
      "\n",
      "\n",
      "=== EPOCH 0/9 ===\n",
      "Learning Rate = 0.0005\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  21%|        | 70/330 [00:25<01:35,  2.72it/s]\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "Stop right there!",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m Stop right there!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/IPython/core/interactiveshell.py:3516: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    print(\"Starting script\\n\")\n",
    "    logger.info(\"Starting script\\n\")\n",
    "    #print(torch.cuda.is_available())\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb2ee54-7f44-4ec5-a2e7-8b0f7aa7dda7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.figure(figsize=(8,8))\n",
    "fig, axs = plt.subplots(2,3, figsize=(8, 5))\n",
    "fig.suptitle('OPTIM BED Loss & mAP', fontsize=15)\n",
    "\n",
    "plt.subplot(2,3,1)\n",
    "plt.plot(train_total_loss, label=\"train\")\n",
    "plt.plot(val_total_loss, label=\"val\")\n",
    "plt.title(\"Total Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "\n",
    "plt.subplot(2,3,2)\n",
    "plt.plot(train_box_loss, label=\"train\")\n",
    "plt.plot(val_box_loss, label=\"val\")\n",
    "plt.title(\"Box Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "\n",
    "plt.subplot(2,3,3)\n",
    "plt.plot(train_confidence_loss, label=\"train\")\n",
    "plt.plot(val_confidence_loss, label=\"val\")\n",
    "plt.title(\"Confidence Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "\n",
    "plt.subplot(2,3,4)\n",
    "plt.plot(train_noobj_loss, label=\"train\")\n",
    "plt.plot(val_noobj_loss, label=\"val\")\n",
    "plt.title(\"NoObj Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "\n",
    "plt.subplot(2,3,5)\n",
    "plt.plot(train_class_loss, label=\"train\")\n",
    "plt.plot(val_class_loss, label=\"val\")\n",
    "plt.title(\"Class Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "\n",
    "plt.subplot(2,3,6)\n",
    "plt.plot(range(0, EPOCHS+1, 5), [0]+val_mAP, label=\"val mAP\")\n",
    "plt.title(\"Val mAP\")\n",
    "plt.ylim([0,1])\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"mAP:@.50\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(log_path + \"bed_loss_ma.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f942a742-dfe5-4585-af50-f04e4bf6579f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_total_loss, label=\"train\")\n",
    "plt.plot(val_total_loss, label=\"val\")\n",
    "plt.title(\"Total Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.savefig(log_path + \"bed_loss_total.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf82c1e-9bff-4fab-a7c4-c6753eb939f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_box_loss, label=\"train\")\n",
    "#plt.scatter(range(EPOCHS), train_box_loss)\n",
    "plt.plot(val_box_loss, label=\"val\")\n",
    "#plt.scatter(range(EPOCHS), val_box_loss)\n",
    "plt.title(\"Box Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.savefig(log_path + \"bed_loss_box.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3c3a27-76d5-4149-8fec-2f69d4c14958",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_confidence_loss, label=\"train\")\n",
    "plt.plot(val_confidence_loss, label=\"val\")\n",
    "plt.title(\"Confidence Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.savefig(log_path + \"bed_loss_conf.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07af6031-83fd-45fe-bad2-3b813cbea7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_noobj_loss, label=\"train\")\n",
    "plt.plot(val_noobj_loss, label=\"val\")\n",
    "plt.title(\"NoObj Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.ylim([0,50])\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.savefig(log_path + \"bed_loss_noobj.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07633ae-085c-4fdb-abf2-6bcad83ba733",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_class_loss, label=\"train\")\n",
    "plt.plot(val_class_loss, label=\"val\")\n",
    "plt.title(\"Class Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.savefig(log_path + \"bed_loss_class.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8191af27-dd22-4718-ab87-6f118df9e476",
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_class_AP\n",
    "# val_class_precision\n",
    "# val_class_recall\n",
    "\n",
    "smoke_prec = []\n",
    "fire_prec = []\n",
    "for prec in val_class_precision:\n",
    "    smoke_prec.append(prec[0])\n",
    "    fire_prec.append(prec[1])\n",
    "\n",
    "smoke_recall = []\n",
    "fire_recall = []\n",
    "for rec in val_class_recall:\n",
    "    smoke_recall.append(rec[0])\n",
    "    fire_recall.append(rec[1])\n",
    "\n",
    "smoke_ap = []\n",
    "fire_ap = []\n",
    "for ap in val_class_AP:\n",
    "    smoke_ap.append(ap[0])\n",
    "    fire_ap.append(ap[1])\n",
    "\n",
    "fig, axs = plt.subplots(1, 3, figsize=(8, 3))\n",
    "fig.suptitle('OPTIM BED precision, recall, class AP')\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(smoke_prec, label=\"smoke\")\n",
    "plt.plot(fire_prec, label=\"fire\")\n",
    "plt.title(\"Precision\")\n",
    "plt.ylim([0,1])\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(smoke_recall, label=\"smoke\")\n",
    "plt.plot(fire_recall, label=\"fire\")\n",
    "plt.title(\"Recall\")\n",
    "plt.ylim([0,1])\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(smoke_ap, label=\"smoke\")\n",
    "plt.plot(fire_ap, label=\"fire\")\n",
    "plt.title(\"Class AP\")\n",
    "plt.ylim([0,1])\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(log_path + \"bed_precision_recall_classAP.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab478d8e-df7f-4977-a5cb-d050b8de5d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(8, 3))\n",
    "fig.suptitle('Optim BED mAP')\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(train_mAP, label=\"train mAP\")\n",
    "plt.title(\"Train mAP\")\n",
    "plt.ylim([0,1])\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"mAP:@.50\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(val_mAP, label=\"val mAP\")\n",
    "plt.title(\"Val mAP\")\n",
    "plt.ylim([0,1])\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"mAP:@.50\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(log_path + \"bed_mAP.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2503a5a7-422f-4adc-8056-32e5fb781ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(figsize=(5, 4))\n",
    "\n",
    "plt.plot(range(0, EPOCHS+1, 15), [0]+train_mAP, label=\"train mAP\")\n",
    "plt.plot(range(0, EPOCHS+1, 5), [0]+val_mAP, label=\"val mAP\")\n",
    "plt.title(\"Train vs Val mAP:@.50\")\n",
    "plt.xlim([0,EPOCHS])\n",
    "plt.ylim([0,1])\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"mAP:@.50\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "\n",
    "plt.savefig(log_path + \"bed_mAP_together.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dce1602-74b1-46c8-b653-4c1910d15251",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
