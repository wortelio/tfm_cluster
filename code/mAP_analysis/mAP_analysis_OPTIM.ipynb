{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fd8089a1-9aea-4c7b-9c03-e4ce770bd14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import cv2\n",
    "\n",
    "from models_class import BIG_BED, OPTIM_BED\n",
    "from utils import (\n",
    "    yolo2pixel,\n",
    "    nms_yv1,\n",
    "    get_bboxes,\n",
    "    mAP,\n",
    "    plot_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0267249b-0c46-412c-a3c9-9863d855e117",
   "metadata": {},
   "source": [
    "# Folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0f08692a-7366-4769-bd80-820be3f0735c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_dir = '../../ds2fire/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea37aa0c-abe6-4bfe-b5a0-b629dc853f84",
   "metadata": {},
   "source": [
    "### DFire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "16923085-8e02-4ac6-abf1-349413355a8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DFire val dir: ['.ipynb_checkpoints', 'labels', 'images']\n"
     ]
    }
   ],
   "source": [
    "dfire_dir = ds_dir + 'dfire_yolo/'\n",
    "dfire_val_dir = dfire_dir + 'test/'\n",
    "dfire_val_imgs = dfire_val_dir + 'images/'\n",
    "dfire_val_labels = dfire_val_dir + 'labels/'\n",
    "\n",
    "print(f'DFire val dir: {os.listdir(dfire_val_dir)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f996ec-13c7-4716-8ba8-dd79c3e936ba",
   "metadata": {},
   "source": [
    "### DFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3783c236-9ff4-4c27-8a1c-37c7a6fcd9bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DFS val dir: ['labels', 'images']\n"
     ]
    }
   ],
   "source": [
    "dfs_dir = ds_dir + 'dfs_xml/'\n",
    "dfs_val_imgs = dfs_dir + 'images/'\n",
    "dfs_val_labels = dfs_dir + 'labels/'\n",
    "\n",
    "print(f'DFS val dir: {os.listdir(dfs_dir)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81652148-3d99-4bce-ae4d-196ae3e961a3",
   "metadata": {},
   "source": [
    "### DFS Files: seed to replicate training split files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "64d2fbf8-1213-4154-b91f-58180c71bd07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DFS number of samples: 9462\n",
      "DFS test samples: 1893\n"
     ]
    }
   ],
   "source": [
    "# Get all txt file paths in path_annot and sort them\n",
    "dfs_xml_files = sorted(\n",
    "    [\n",
    "        os.path.join(dfs_val_labels, file_name)\n",
    "        for file_name in os.listdir(dfs_val_labels)\n",
    "        if file_name.endswith(\".xml\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "dfs_len = len(dfs_xml_files)\n",
    "dfs_train_elements = int(dfs_len*0.8)\n",
    "dfs_test_elements = dfs_len - dfs_train_elements\n",
    "\n",
    "random.seed(123)\n",
    "random.shuffle(dfs_xml_files)\n",
    "dfs_test_list = dfs_xml_files[dfs_train_elements:]\n",
    "\n",
    "print(f'DFS number of samples: {dfs_len}')\n",
    "print(f'DFS test samples: {len(dfs_test_list)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea17edca-4cff-48f3-bad5-6ab846d1ab7e",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6f73f679-c99f-4dbe-8413-2fe358655ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASSES = [\"smoke\", \"fire\"]\n",
    "\n",
    "IMG_DIM = {'W':224, 'H':224} # (W, H)\n",
    "IMG_W = IMG_DIM['W']\n",
    "IMG_H = IMG_DIM['H']\n",
    "\n",
    "SX = 7\n",
    "SY = 7\n",
    "B = 2 # Number of bounding boxes to predict.\n",
    "C = len(CLASSES) # Number of classes in the dataset.\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "WORKERS = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b119e5c6-8df6-4c42-8c6e-ef01945c82b8",
   "metadata": {},
   "source": [
    "# DFire Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "34bf5ab7-85c0-41e9-ab7e-1e48f58c0c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DFireDataset(Dataset):\n",
    "    '''\n",
    "    Creates a Pytorch Dataset to train the Yolov1 Network.\n",
    "    Encodes labels to match the format [xcell, ycell, w, h, confidence, class_0 (smoke), class_1 (fire)]\n",
    "        - Final encoding format is: [xcell, ycell, w, h, conf=1, smoke?, fire?]\n",
    "\n",
    "    Discard images when there are more than 1 object in the same cell\n",
    "    \n",
    "    Arguments:\n",
    "        - img_h:            image height\n",
    "        - img_w:            image width\n",
    "        - img_dir:          path to images folder\n",
    "        - label_dir:        path to labels folder\n",
    "        - SX:               number of cells in X axis (horizontal -> width)\n",
    "        - SY:               number of cells in Y axis (vertical -> height)\n",
    "        - C:                number of classes, 2 in this case\n",
    "        - transform:        transformation applied to input images -> Albumentations\n",
    "        - target_transform: transformation applied to labels -> nothing by default\n",
    "\n",
    "    Return:\n",
    "        - img:              1 image of the dataset\n",
    "        - target:           corresponding label encoded\n",
    "    '''\n",
    "\n",
    "    def __init__(self, img_h, img_w, img_dir, label_dir, \n",
    "                 SX, SY, C, \n",
    "                 transform=None, target_transform=None):\n",
    "        self.img_h = img_h\n",
    "        self.img_w = img_w\n",
    "        self.img_dir = img_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.SX = SX\n",
    "        self.SY = SY\n",
    "        self.C = C\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "        self.labels_list = sorted(\n",
    "            [\n",
    "                os.path.join(self.label_dir, file_name)\n",
    "                for file_name in os.listdir(self.label_dir)\n",
    "                if file_name.endswith(\".txt\")\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        self.images, self.bboxes, self.labels = self.__build_ds__(self.labels_list)\n",
    "        \n",
    "        self.num_samples = self.images.shape[0]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "    \n",
    "    def __bbox_check__(self, bbox):\n",
    "        eps = 1e-6\n",
    "        \n",
    "        xc, yc, w, h = bbox[0], bbox[1], bbox[2], bbox[3]\n",
    "        xmin = xc - w/2\n",
    "        ymin = yc - h/2\n",
    "        xmax = xc + w/2\n",
    "        ymax = yc + h/2\n",
    "        \n",
    "        xmin = max(xmin, 0 + eps)\n",
    "        ymin = max(ymin, 0 + eps)\n",
    "        xmax = min(xmax, 1)\n",
    "        ymax = min(ymax, 1)\n",
    "        \n",
    "        bbox = np.array([ \n",
    "                (xmin+xmax)/2,\n",
    "                (ymin+ymax)/2,\n",
    "                xmax-xmin,\n",
    "                ymax-ymin\n",
    "                 ]).astype(np.float32)\n",
    "        \n",
    "        return bbox        \n",
    "\n",
    "\n",
    "    def __build_ds__(self, labels_list):\n",
    "        bboxes = []\n",
    "        labels = []\n",
    "        images = []\n",
    "        wrong_imgs = 0\n",
    "        overlapping_rem = 0\n",
    "        more_than_3 = 0\n",
    "                \n",
    "        for label in labels_list:\n",
    "            fname = Path(label).stem\n",
    "            image_path = self.img_dir + fname + '.jpg'   \n",
    "            #print(fname, image_path)\n",
    "                                   \n",
    "            if cv2.imread(image_path) is None:\n",
    "                print(f'{image_path} cannot be read by cv2 -> removed')\n",
    "                wrong_imgs += 1\n",
    "            \n",
    "            else:\n",
    "                \n",
    "                label_mtx = np.zeros((self.SY, self.SX))\n",
    "                overlapping_object = 0\n",
    "\n",
    "                one_bboxes = []\n",
    "                one_labels = []\n",
    "            \n",
    "                with open(label) as f:\n",
    "                    lines = f.readlines()\n",
    "\n",
    "                    # Restrict to 3 boxes per sample\n",
    "                    if len(lines) > 3:\n",
    "                        more_than_3 += 1\n",
    "                        continue\n",
    "                        \n",
    "                    for line in lines:\n",
    "                        class_id, x, y, w, h = line.strip().split()\n",
    "                        class_id = int(class_id)\n",
    "                        box = np.array([x, y, w, h]).astype(np.float32)\n",
    "                        x, y, w, h = box[0], box[1], box[2], box[3]\n",
    "                        box_ok = self.__bbox_check__([x, y, w, h])\n",
    "                        x, y, w, h = box_ok[0], box_ok[1], box_ok[2], box_ok[3]\n",
    "                        i, j = math.floor(y * self.SY), math.floor(x * self.SX)\n",
    "                        if label_mtx[i, j] == 1:\n",
    "                            overlapping_object = 1\n",
    "                            overlapping_rem += 1\n",
    "                            #print(f'Removed {label} due to overlapping object in cell {i, j}')\n",
    "                            break\n",
    "                        else:\n",
    "                            label_mtx[i, j] = 1\n",
    "                            one_bboxes.append([x, y, w, h])\n",
    "                            # smoke\n",
    "                            if class_id == 0:\n",
    "                                one_labels.append(0)\n",
    "                            # fire\n",
    "                            elif class_id == 1:\n",
    "                                one_labels.append(1)\n",
    "                            else:\n",
    "                                print(f'File {label} errored in cell {i, j}')\n",
    "\n",
    "                    if overlapping_object == 0:\n",
    "                        # Padding to SX*SY labels and bounding boxes, so you can store tensors\n",
    "                        # Label -1 indicates no box\n",
    "                        for idx in range(self.SX*self.SY - len(one_labels)):\n",
    "                            one_bboxes.append([0, 0, 0, 0])\n",
    "                            one_labels.append(-1)\n",
    "                        # print(f'\\nBboxes and Labels of image {image_path}')\n",
    "                        # print(\"Bboxes\")\n",
    "                        # for box in one_bboxes:\n",
    "                        #     print(box)\n",
    "                        # print(\"Labels\")\n",
    "                        # for label in one_labels:\n",
    "                        #     print(label)\n",
    "                        bboxes.append(one_bboxes)\n",
    "                        labels.append(one_labels)\n",
    "                        images.append(image_path)\n",
    "        \n",
    "        print(f'Removed wrong images: {wrong_imgs}')\n",
    "        print(f'Removed due to overlapping: {overlapping_rem}')\n",
    "        print(f'Removed due to more than 3: {more_than_3}')\n",
    "\n",
    "        labels_np = np.array(labels)\n",
    "        labels_tensor = torch.tensor(labels_np, dtype=torch.float32)\n",
    "        bboxes_np = np.array(bboxes)\n",
    "        bboxes_tensor = torch.tensor(bboxes_np, dtype=torch.float32)\n",
    "        images_array = np.array(images)\n",
    "        # print(f'Images array {images_array}')\n",
    "        # print(f'Bboxes tensor {bboxes_tensor}')\n",
    "        # print(f'Labels tensor {labels_tensor}')\n",
    "        \n",
    "        return images_array, bboxes_tensor, labels_tensor\n",
    "        #return images, bboxes, labels\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        # Image processing\n",
    "        img_file = self.images[index]\n",
    "        img = cv2.imread(img_file)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)   \n",
    "        #img = cv2.resize(img, (self.img_w, self.img_h), interpolation = cv2.INTER_NEAREST)\n",
    "\n",
    "        # Labels processing\n",
    "        bboxes = self.bboxes[index]\n",
    "        bboxes = bboxes[~torch.all(bboxes == torch.tensor([0,0,0,0]), dim=1)]\n",
    "        bboxes = bboxes.numpy().tolist()\n",
    "        #print(bboxes)\n",
    "        labels = self.labels[index]\n",
    "        labels = labels[labels != -1.]\n",
    "        labels = labels.numpy().tolist()\n",
    "        #print(f'Labels inside dataset {labels}')\n",
    "        \n",
    "        # Data Augmentation\n",
    "        if self.transform is not None:\n",
    "            try:\n",
    "                aug = self.transform(image=img, bboxes=bboxes, class_labels=labels)\n",
    "                img = aug['image']\n",
    "                bboxes = aug['bboxes']\n",
    "                labels = aug['class_labels']\n",
    "            except:\n",
    "                #print(f'Error trying to augment image {img_file}')\n",
    "                img = cv2.resize(img, (self.img_w, self.img_h), interpolation = cv2.INTER_NEAREST)\n",
    "                img = (img / 255.) - 0.5 # Ocurrencia de flipao\n",
    "                img = torch.tensor(img, dtype=torch.float32)\n",
    "                img = img.permute(2, 0, 1)\n",
    "        \n",
    "        label_mtx = np.zeros((self.SY, self.SX, 5+self.C))\n",
    "        \n",
    "        for box, label in zip(bboxes, labels):\n",
    "            class_id = int(label)\n",
    "            i, j = int(box[1]*self.SY), int(box[0]*self.SX)\n",
    "            xcell, ycell = box[0]*self.SX - j, box[1]*self.SY - i\n",
    "            label_mtx[i, j, :5] = [xcell, ycell, box[2], box[3], 1]\n",
    "            label_mtx[i, j, 5+class_id] = 1\n",
    "\n",
    "        label_mtx = torch.tensor(label_mtx, dtype=torch.float32)\n",
    "        \n",
    "        #return img, label_mtx, img_file\n",
    "        return img, label_mtx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f02472-616b-4465-b3da-1384ad03ccac",
   "metadata": {},
   "source": [
    "# DFS Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c95e5473-d5e8-41db-8be5-816eea26d50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DFSDataset(Dataset):\n",
    "    '''\n",
    "    Creates a Pytorch Dataset to train the Yolov1 Network.\n",
    "    Encodes labels to match the format [xcell, ycell, w, h, confidence, class_0 (smoke), class_1 (fire)]\n",
    "        - Final encoding format is: [xcell, ycell, w, h, conf=1, smoke?, fire?]\n",
    "\n",
    "    Discard images when there are more than 1 object in the same cell\n",
    "    \n",
    "    Arguments:\n",
    "        - img_h:            image height\n",
    "        - img_w:            image width\n",
    "        - img_dir:          path to images folder\n",
    "        - label_dir:        path to labels folder\n",
    "        - SX:               number of cells in X axis (horizontal -> width)\n",
    "        - SY:               number of cells in Y axis (vertical -> height)\n",
    "        - C:                number of classes, 2 in this case\n",
    "        - transform:        transformation applied to input images -> Albumentations\n",
    "        - target_transform: transformation applied to labels -> nothing by default\n",
    "\n",
    "    Return:\n",
    "        - img:              1 image of the dataset\n",
    "        - target:           corresponding label encoded\n",
    "    '''\n",
    "\n",
    "    def __init__(self, img_h, img_w, img_dir, labels_list, \n",
    "                 SX, SY, C, \n",
    "                 transform=None, target_transform=None):\n",
    "        self.img_h = img_h\n",
    "        self.img_w = img_w\n",
    "        self.img_dir = img_dir\n",
    "        self.labels_list = labels_list\n",
    "        self.SX = SX\n",
    "        self.SY = SY\n",
    "        self.C = C\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        \n",
    "        self.images, self.bboxes, self.labels = self.__build_ds__(self.labels_list)\n",
    "        \n",
    "        self.num_samples = self.images.shape[0]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "    \n",
    "    def __bbox_check__(self, bbox):\n",
    "        eps = 1e-6\n",
    "        \n",
    "        xmin, ymin, xmax, ymax = bbox[0], bbox[1], bbox[2], bbox[3]\n",
    "        \n",
    "        xmin = max(xmin, 0 + eps)\n",
    "        ymin = max(ymin, 0 + eps)\n",
    "        xmax = min(xmax, 1)\n",
    "        ymax = min(ymax, 1)\n",
    "        \n",
    "        bbox = np.array([ \n",
    "                (xmin+xmax)/2,\n",
    "                (ymin+ymax)/2,\n",
    "                xmax-xmin,\n",
    "                ymax-ymin\n",
    "                 ]).astype(np.float32)\n",
    "        \n",
    "        return bbox        \n",
    "\n",
    "\n",
    "    def __build_ds__(self, labels_list):\n",
    "        bboxes = []\n",
    "        labels = []\n",
    "        images = []\n",
    "        wrong_imgs = 0\n",
    "        overlapping_rem = 0\n",
    "        more_than_3 = 0\n",
    "        \n",
    "        \n",
    "        for xml_file in labels_list:\n",
    "#             fname = Path(xml_file).stem\n",
    "#             image_path = self.img_dir + fname + '.jpg'   \n",
    "            #print(fname, image_path)\n",
    "            \n",
    "            tree = ET.parse(xml_file)\n",
    "            root = tree.getroot()\n",
    "\n",
    "            image_name = root.find(\"filename\").text\n",
    "            image_path = os.path.join(self.img_dir, image_name)\n",
    "            #print(image_name, image_path)\n",
    "            \n",
    "            if cv2.imread(image_path) is None:\n",
    "                print(f'{image_path} cannot be read by cv2 -> removed')\n",
    "                wrong_imgs += 1\n",
    "            \n",
    "            else:\n",
    "                overlapping_object = 0\n",
    "                more_than_3_in = 0\n",
    "\n",
    "                label_mtx = np.zeros((self.SY, self.SX))\n",
    "\n",
    "                size = root.find(\"size\")\n",
    "                img_w = float(size.find(\"width\").text)\n",
    "                img_h = float(size.find(\"height\").text)\n",
    "\n",
    "                one_bboxes = []\n",
    "                one_labels = []\n",
    "\n",
    "                for obj in root.iter(\"object\"):\n",
    "                    class_name = obj.find(\"name\").text\n",
    "                    if class_name == 'smoke':\n",
    "                        class_id = 0\n",
    "                        more_than_3_in += 1\n",
    "                    elif class_name == 'fire':\n",
    "                        class_id = 1\n",
    "                        more_than_3_in += 1\n",
    "                    else:\n",
    "                        continue \n",
    "                    bbox = obj.find(\"bndbox\")\n",
    "                    xmin = float(bbox.find(\"xmin\").text)/img_w\n",
    "                    ymin = float(bbox.find(\"ymin\").text)/img_h\n",
    "                    xmax = float(bbox.find(\"xmax\").text)/img_w\n",
    "                    ymax = float(bbox.find(\"ymax\").text)/img_h\n",
    "\n",
    "                    box = self.__bbox_check__([xmin, ymin, xmax, ymax])\n",
    "                    #print(f'Class: {class_name} - Class_id: {class_id}. Coords: x={x}, y={y}, w={w}, h={h}')\n",
    "\n",
    "                    x, y, w, h = box[0], box[1], box[2], box[3]\n",
    "                    i, j = math.floor(y * self.SY), math.floor(x * self.SX)\n",
    "                    if label_mtx[i, j] == 1:\n",
    "                        overlapping_object = 1\n",
    "                        overlapping_rem += 1\n",
    "                        break\n",
    "                    else:\n",
    "                        label_mtx[i, j] = 1\n",
    "                        one_bboxes.append([x, y, w, h])\n",
    "                        # smoke\n",
    "                        if class_id == 0:\n",
    "                            one_labels.append(0)\n",
    "                        # fire\n",
    "                        elif class_id == 1:\n",
    "                            one_labels.append(1)\n",
    "                        else:\n",
    "                            print(f'File {label} errored in cell {i, j}') \n",
    "                \n",
    "                if more_than_3_in > 3:\n",
    "                    more_than_3 += 1\n",
    "                    continue\n",
    "                \n",
    "                if (overlapping_object == 0) and (more_than_3_in < 4):\n",
    "                    # Padding to SX*SY labels and bounding boxes, so you can store tensors\n",
    "                    # Label -1 indicates no box\n",
    "                    for idx in range(self.SX*self.SY - len(one_labels)):\n",
    "                        one_bboxes.append([0, 0, 0, 0])\n",
    "                        one_labels.append(-1)\n",
    "                    # print(f'\\nBboxes and Labels of image {image_path}')\n",
    "                    # print(\"Bboxes\")\n",
    "                    # for box in one_bboxes:\n",
    "                    #     print(box)\n",
    "                    # print(\"Labels\")\n",
    "                    # for label in one_labels:\n",
    "                    #     print(label)\n",
    "                    bboxes.append(one_bboxes)\n",
    "                    labels.append(one_labels)\n",
    "                    images.append(image_path)\n",
    "\n",
    "        print(f'Removed wrong images: {wrong_imgs}')\n",
    "        print(f'Removed due to overlapping: {overlapping_rem}')\n",
    "        print(f'Removed due to more than 3: {more_than_3}')\n",
    "\n",
    "        labels_np = np.array(labels)\n",
    "        labels_tensor = torch.tensor(labels_np, dtype=torch.float32)\n",
    "        bboxes_np = np.array(bboxes)\n",
    "        bboxes_tensor = torch.tensor(bboxes_np, dtype=torch.float32)\n",
    "        images_array = np.array(images)\n",
    "        # print(f'Images array {images_array}')\n",
    "        # print(f'Bboxes tensor {bboxes_tensor}')\n",
    "        # print(f'Labels tensor {labels_tensor}')\n",
    "        \n",
    "        return images_array, bboxes_tensor, labels_tensor\n",
    "        #return images, bboxes, labels\n",
    "\n",
    "\n",
    " \n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        # Image processing\n",
    "        img_file = self.images[index]\n",
    "        img = cv2.imread(img_file)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)   \n",
    "        #img = cv2.resize(img, (self.img_w, self.img_h), interpolation = cv2.INTER_NEAREST)\n",
    "\n",
    "        # Labels processing\n",
    "        bboxes = self.bboxes[index]\n",
    "        bboxes = bboxes[~torch.all(bboxes == torch.tensor([0,0,0,0]), dim=1)]\n",
    "        bboxes = bboxes.numpy().tolist()\n",
    "        #print(bboxes)\n",
    "        labels = self.labels[index]\n",
    "        labels = labels[labels != -1.]\n",
    "        labels = labels.numpy().tolist()\n",
    "        #print(f'Labels inside dataset {labels}')\n",
    "        \n",
    "        # Data Augmentation\n",
    "        if self.transform is not None:\n",
    "            try:\n",
    "                aug = self.transform(image=img, bboxes=bboxes, class_labels=labels)\n",
    "                img = aug['image']\n",
    "                bboxes = aug['bboxes']\n",
    "                labels = aug['class_labels']\n",
    "            except:\n",
    "                #print(f'Error trying to augment image {img_file}')\n",
    "                img = cv2.resize(img, (self.img_w, self.img_h), interpolation = cv2.INTER_NEAREST)\n",
    "                img = (img / 255.) - 0.5 # Ocurrencia de flipao\n",
    "                img = torch.tensor(img, dtype=torch.float32)\n",
    "                img = img.permute(2, 0, 1)\n",
    "        \n",
    "        label_mtx = np.zeros((self.SY, self.SX, 5+self.C))\n",
    "        \n",
    "        for box, label in zip(bboxes, labels):\n",
    "            class_id = int(label)\n",
    "            i, j = int(box[1]*self.SY), int(box[0]*self.SX)\n",
    "            xcell, ycell = box[0]*self.SX - j, box[1]*self.SY - i\n",
    "            label_mtx[i, j, :5] = [xcell, ycell, box[2], box[3], 1]\n",
    "            label_mtx[i, j, 5+class_id] = 1\n",
    "\n",
    "        label_mtx = torch.tensor(label_mtx, dtype=torch.float32)\n",
    "        \n",
    "        #return img, label_mtx, img_file\n",
    "        return img, label_mtx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877a1ea7-30d2-479b-8813-dce909ac84d9",
   "metadata": {},
   "source": [
    "# Preprocess, Datasets and DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c247aac6-0203-4275-8fa7-365fa1950c48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed wrong images: 0\n",
      "Removed due to overlapping: 118\n",
      "Removed due to more than 3: 445\n",
      "Removed wrong images: 0\n",
      "Removed due to overlapping: 58\n",
      "Removed due to more than 3: 277\n",
      "Test dataset len: 5330\n"
     ]
    }
   ],
   "source": [
    "val_transform = A.Compose([\n",
    "    A.Resize(IMG_H, IMG_W, p=1),\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), p=1),\n",
    "    ToTensorV2(p=1),\n",
    "], bbox_params=A.BboxParams(format='yolo', label_fields=['class_labels']))\n",
    "\n",
    "dfire_val_dataset = DFireDataset(img_h = IMG_H,\n",
    "                                 img_w = IMG_W,\n",
    "                                 img_dir = dfire_val_imgs,\n",
    "                                 label_dir = dfire_val_labels,\n",
    "                                 SX = SX,\n",
    "                                 SY = SY,\n",
    "                                 C = C,\n",
    "                                 transform=val_transform)\n",
    "\n",
    "dfs_val_dataset = DFSDataset(img_h = IMG_H,\n",
    "                             img_w = IMG_W,\n",
    "                             img_dir = dfs_val_imgs,\n",
    "                             labels_list = dfs_test_list,\n",
    "                             SX = SX,\n",
    "                             SY = SY,\n",
    "                             C = C,\n",
    "                             transform=val_transform)\n",
    "\n",
    "full_test_ds = torch.utils.data.ConcatDataset((dfire_val_dataset, dfs_val_dataset))\n",
    "print(f'Test dataset len: {len(full_test_ds)}')\n",
    "\n",
    "val_loader = DataLoader(dataset=full_test_ds,\n",
    "                        batch_size=BATCH_SIZE,\n",
    "                        num_workers=WORKERS,\n",
    "                        pin_memory=True,\n",
    "                        shuffle=False,\n",
    "                        drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17ae0b1-24e1-4394-92f3-a89b92abe17b",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "635819cd-31ee-43a3-b49d-ae44f10b465e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c7f16b61-da8f-4499-bae9-509609963202",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OPTIM_BED(\n",
       "  (model): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (4): Conv2d(64, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (5): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "    (6): ReLU()\n",
       "    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (8): Conv2d(24, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (9): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "    (10): ReLU()\n",
       "    (11): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (12): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "    (13): ReLU()\n",
       "    (14): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (15): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "    (16): ReLU()\n",
       "    (17): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (18): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "    (19): ReLU()\n",
       "    (20): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (21): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (22): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "    (23): ReLU()\n",
       "    (24): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (25): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "    (26): ReLU()\n",
       "    (27): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (28): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "    (29): ReLU()\n",
       "    (30): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (31): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "    (32): ReLU()\n",
       "    (33): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (34): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "    (35): ReLU()\n",
       "    (36): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (37): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "    (38): ReLU()\n",
       "    (39): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (40): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (41): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "    (42): ReLU()\n",
       "    (43): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (44): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "    (45): ReLU()\n",
       "    (46): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (47): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "    (48): ReLU()\n",
       "    (49): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (50): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "    (51): ReLU()\n",
       "    (52): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (53): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "    (54): ReLU()\n",
       "    (55): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (56): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "    (57): ReLU()\n",
       "    (58): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (59): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (60): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "    (61): ReLU()\n",
       "    (62): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (63): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "    (64): ReLU()\n",
       "    (65): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (66): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "    (67): ReLU()\n",
       "    (68): Conv2d(128, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (69): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "    (70): ReLU()\n",
       "    (71): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (72): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "    (73): ReLU()\n",
       "    (74): Conv2d(16, 12, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = OPTIM_BED(num_classes=C, S=SX, B=B, in_channels=3).to(DEVICE)\n",
    "model_path = '../../bed_pytorch_results/optim/'\n",
    "model_file = model_path + 'bed_best_2BB_mAP.pt'\n",
    "\n",
    "checkpoint = torch.load(model_file, map_location=torch.device(DEVICE))\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "model.eval() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fcaf0b0-e8e0-4e42-a266-89961a6b6247",
   "metadata": {},
   "source": [
    "# Calculate mAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b11bb1b7-adb5-44b0-93b3-6ceb036f8922",
   "metadata": {},
   "outputs": [],
   "source": [
    "cell2box_mask = torch.zeros((SY, SX, 2))\n",
    "for i in range(SY):\n",
    "    for j in range(SX):\n",
    "        cell2box_mask[i,j,0] = j\n",
    "        cell2box_mask[i,j,1] = i "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "290e95fd-6aad-4d3c-8789-9dec7eee9262",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mAP(thres, iou_nms, iou_map, loader, model, mask):\n",
    "    pred_boxes, target_boxes = get_bboxes(loader=loader, \n",
    "                                      model=model,\n",
    "                                      SX=SX,\n",
    "                                      SY=SY,\n",
    "                                      B=B,\n",
    "                                      C=C,\n",
    "                                      mask=mask,\n",
    "                                      iou_threshold=iou_nms, \n",
    "                                      threshold=thres,\n",
    "                                      device=DEVICE,\n",
    "                                      box_format=\"midpoint\")\n",
    "\n",
    "    mean_avg_prec, avg_prec, cls_prec, cls_rec = mAP(pred_boxes=pred_boxes, \n",
    "                                                     true_boxes=target_boxes, \n",
    "                                                     iou_threshold=iou_map, \n",
    "                                                     box_format=\"midpoint\",\n",
    "                                                     num_classes=C)\n",
    "    # print(f'mAP: {mean_avg_prec}')\n",
    "    # print(f'smoke AP: {avg_prec[0]}')\n",
    "    # print(f'fire AP: {avg_prec[1]}')\n",
    "    # print(f'smoke precision: {cls_prec[0]}')\n",
    "    # print(f'fire precision: {cls_prec[1]}')\n",
    "    # print(f'smoke recall: {cls_rec[0]}')\n",
    "    # print(f'fire recall: {cls_rec[1]}')\n",
    "\n",
    "    f1_smoke = 2*(cls_prec[0]*cls_rec[0])/(cls_prec[0]+cls_rec[0])\n",
    "    f1_fire = 2*(cls_prec[1]*cls_rec[1])/(cls_prec[1]+cls_rec[1])\n",
    "    f1_mean = (f1_smoke + f1_fire) / 2\n",
    "\n",
    "    print(\"mAP\".ljust(9) + \"|\" + \n",
    "          \"AP smoke\".ljust(9) + \"|\" + \n",
    "          \"AP fire\".ljust(9) + \"|\" + \n",
    "          \"F1 mean\".ljust(9) + \"|\" + \n",
    "          \"F1 smoke\".ljust(9) + \"|\" + \n",
    "          \"F1 fire\".ljust(9) + \"|\" +\n",
    "          \"Pr smoke\".ljust(9) + \"|\" +\n",
    "          \"Pr fire\".ljust(9) + \"|\" +\n",
    "          \"Re smoke\".ljust(9) + \"|\" +\n",
    "          \"Re fire\".ljust(9))\n",
    "\n",
    "    print_hyphen = \"\"\n",
    "    for i in range(10): # 10 = # of metrics\n",
    "        print_hyphen += \"---------\".ljust(9) + \" \"\n",
    "    print(print_hyphen)\n",
    "    \n",
    "    print(f'{mean_avg_prec:.3f}'.ljust(9) + \"|\" +\n",
    "          f'{avg_prec[0]:.3f}'.ljust(9) + \"|\" +\n",
    "          f'{avg_prec[1]:.3f}'.ljust(9) + \"|\" +\n",
    "          f'{f1_mean:.3f}'.ljust(9) + \"|\" +\n",
    "          f'{f1_smoke:.3f}'.ljust(9) + \"|\" +\n",
    "          f'{f1_fire:.3f}'.ljust(9) + \"|\" +\n",
    "          f'{cls_prec[0]:.3f}'.ljust(9) + \"|\" +\n",
    "          f'{cls_prec[1]:.3f}'.ljust(9) + \"|\" +\n",
    "          f'{cls_rec[0]:.3f}'.ljust(9) + \"|\" +\n",
    "          f'{cls_rec[1]:.3f}'.ljust(9) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "43e1f333-7fe2-4a72-b9cf-99e9b61f2d68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Get Boxes:   0%|          | 0/83 [00:00<?, ?it/s]/opt/conda/lib/python3.11/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/opt/conda/lib/python3.11/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "Get Boxes: 100%|██████████| 83/83 [00:24<00:00,  3.45it/s]\n",
      "mAP:@.5: 100%|██████████| 2/2 [00:02<00:00,  1.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mAP      |AP smoke |AP fire  |F1 mean  |F1 smoke |F1 fire  |Pr smoke |Pr fire  |Re smoke |Re fire  \n",
      "--------- --------- --------- --------- --------- --------- --------- --------- --------- --------- \n",
      "0.432    |0.355    |0.509    |0.562    |0.518    |0.606    |0.541    |0.572    |0.496    |0.645    \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "calculate_mAP(thres=0.2,\n",
    "              iou_nms=0.3,\n",
    "              iou_map=0.5,\n",
    "              loader=val_loader,\n",
    "              model=model,\n",
    "              mask=cell2box_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cdf97d5b-b838-424c-a06e-dd648074b82d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Get Boxes:   0%|          | 0/83 [00:00<?, ?it/s]/opt/conda/lib/python3.11/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/opt/conda/lib/python3.11/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "Get Boxes: 100%|██████████| 83/83 [00:21<00:00,  3.86it/s]\n",
      "mAP:@.5: 100%|██████████| 2/2 [00:02<00:00,  1.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mAP      |AP smoke |AP fire  |F1 mean  |F1 smoke |F1 fire  |Pr smoke |Pr fire  |Re smoke |Re fire  \n",
      "--------- --------- --------- --------- --------- --------- --------- --------- --------- --------- \n",
      "0.438    |0.359    |0.517    |0.553    |0.512    |0.594    |0.516    |0.541    |0.507    |0.659    \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "calculate_mAP(thres=0.2,\n",
    "              iou_nms=0.5,\n",
    "              iou_map=0.5,\n",
    "              loader=val_loader,\n",
    "              model=model,\n",
    "              mask=cell2box_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "af7ab0ab-c78f-4251-bd17-61399261910b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Get Boxes:   0%|          | 0/83 [00:00<?, ?it/s]/opt/conda/lib/python3.11/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/opt/conda/lib/python3.11/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "Get Boxes: 100%|██████████| 83/83 [00:28<00:00,  2.89it/s]\n",
      "mAP:@.5: 100%|██████████| 2/2 [00:02<00:00,  1.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mAP      |AP smoke |AP fire  |F1 mean  |F1 smoke |F1 fire  |Pr smoke |Pr fire  |Re smoke |Re fire  \n",
      "--------- --------- --------- --------- --------- --------- --------- --------- --------- --------- \n",
      "0.455    |0.383    |0.528    |0.486    |0.449    |0.523    |0.375    |0.424    |0.560    |0.682    \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "calculate_mAP(thres=0.1,\n",
    "              iou_nms=0.5,\n",
    "              iou_map=0.5,\n",
    "              loader=val_loader,\n",
    "              model=model,\n",
    "              mask=cell2box_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "34650d18-258b-4a50-964d-18a672c875a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Get Boxes:   0%|          | 0/83 [00:00<?, ?it/s]/opt/conda/lib/python3.11/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/opt/conda/lib/python3.11/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "Get Boxes: 100%|██████████| 83/83 [00:20<00:00,  4.07it/s]\n",
      "mAP:@.5: 100%|██████████| 2/2 [00:01<00:00,  1.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mAP      |AP smoke |AP fire  |F1 mean  |F1 smoke |F1 fire  |Pr smoke |Pr fire  |Re smoke |Re fire  \n",
      "--------- --------- --------- --------- --------- --------- --------- --------- --------- --------- \n",
      "0.560    |0.469    |0.651    |0.648    |0.601    |0.695    |0.628    |0.655    |0.576    |0.739    \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "calculate_mAP(thres=0.2,\n",
    "              iou_nms=0.3,\n",
    "              iou_map=0.4,\n",
    "              loader=val_loader,\n",
    "              model=model,\n",
    "              mask=cell2box_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a7157629-8e15-4e51-bfe1-4359ed9ae164",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Get Boxes:   0%|          | 0/83 [00:00<?, ?it/s]/opt/conda/lib/python3.11/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/opt/conda/lib/python3.11/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "Get Boxes: 100%|██████████| 83/83 [00:20<00:00,  4.02it/s]\n",
      "mAP:@.5: 100%|██████████| 2/2 [00:01<00:00,  1.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mAP      |AP smoke |AP fire  |F1 mean  |F1 smoke |F1 fire  |Pr smoke |Pr fire  |Re smoke |Re fire  \n",
      "--------- --------- --------- --------- --------- --------- --------- --------- --------- --------- \n",
      "0.650    |0.559    |0.740    |0.706    |0.661    |0.750    |0.691    |0.708    |0.633    |0.798    \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "calculate_mAP(thres=0.2,\n",
    "              iou_nms=0.3,\n",
    "              iou_map=0.3,\n",
    "              loader=val_loader,\n",
    "              model=model,\n",
    "              mask=cell2box_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9580e734-da19-4831-b195-f1696b8b998e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Get Boxes:   0%|          | 0/83 [00:00<?, ?it/s]/opt/conda/lib/python3.11/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/opt/conda/lib/python3.11/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "Get Boxes: 100%|██████████| 83/83 [00:20<00:00,  3.98it/s]\n",
      "mAP:@.5: 100%|██████████| 2/2 [00:02<00:00,  1.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mAP      |AP smoke |AP fire  |F1 mean  |F1 smoke |F1 fire  |Pr smoke |Pr fire  |Re smoke |Re fire  \n",
      "--------- --------- --------- --------- --------- --------- --------- --------- --------- --------- \n",
      "0.589    |0.507    |0.672    |0.560    |0.524    |0.596    |0.438    |0.483    |0.654    |0.777    \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "calculate_mAP(thres=0.1,\n",
    "              iou_nms=0.5,\n",
    "              iou_map=0.4,\n",
    "              loader=val_loader,\n",
    "              model=model,\n",
    "              mask=cell2box_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8c7c437e-888b-4e57-84f9-dafd663e8204",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Get Boxes:   0%|          | 0/83 [00:00<?, ?it/s]/opt/conda/lib/python3.11/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/opt/conda/lib/python3.11/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "Get Boxes: 100%|██████████| 83/83 [00:20<00:00,  3.99it/s]\n",
      "mAP:@.5: 100%|██████████| 2/2 [00:02<00:00,  1.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mAP      |AP smoke |AP fire  |F1 mean  |F1 smoke |F1 fire  |Pr smoke |Pr fire  |Re smoke |Re fire  \n",
      "--------- --------- --------- --------- --------- --------- --------- --------- --------- --------- \n",
      "0.683    |0.606    |0.761    |0.611    |0.580    |0.641    |0.484    |0.520    |0.724    |0.837    \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "calculate_mAP(thres=0.1,\n",
    "              iou_nms=0.5,\n",
    "              iou_map=0.3,\n",
    "              loader=val_loader,\n",
    "              model=model,\n",
    "              mask=cell2box_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f249e85d-7adb-4d24-8bb0-07bef9ce8203",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
